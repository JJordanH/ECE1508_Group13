{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11267503,"sourceType":"datasetVersion","datasetId":7043210},{"sourceId":11270112,"sourceType":"datasetVersion","datasetId":7044920},{"sourceId":11292906,"sourceType":"datasetVersion","datasetId":7061077},{"sourceId":11407585,"sourceType":"datasetVersion","datasetId":7145806},{"sourceId":11407645,"sourceType":"datasetVersion","datasetId":7145855}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --user -q transformers accelerate bitsandbytes \\\ndatasets evaluate sentence-transformers faiss-gpu langchain \\\nlangchain-community openpyxl pacmap ragatouille langchain-huggingface rank_bm25 gdown\n\n!pip install -U ipywidgets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"PATH\"] += \":/root/.local/bin\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U langchain-community\n# !pip install vllm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 降级transformer\n# !pip install transformers==4.30.2\n!pip install vllm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import standard Python tools\nimport os\nimport io\nimport pickle\nimport contextlib\nfrom typing import Optional, List, Tuple\nimport json\nimport torch\n# from rank_bm25 import BM25Okapi\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Import libraries for data handling and visualization\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\n\n# Configure pandas for better visualization of retriever outputs\npd.set_option(\"display.max_colwidth\", None)\n\n# Import text processing and embeddings tools\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import (\n    pipeline,\n    Pipeline,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    AutoModelForQuestionAnswering,\n    BitsAndBytesConfig,\n    DefaultDataCollator,\n    TrainingArguments,\n    Trainer,\n)\n\n# Import LangChain tools\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\nfrom langchain.docstore.document import Document as LangchainDocument\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\n# from langchain_community.vectorstores.utils import DistanceStrategy\n\n# Import RAG-specific tools\n# from ragatouille import RAGPretrainedModel\n\n# Import tools for datasets\nfrom datasets import Dataset\n\n# Advanced visualization and dimensionality reduction tools\nimport pacmap\n\n# Progress bar for loops\nfrom tqdm.notebook import tqdm\n\n# Import vLLM\nfrom vllm import LLM, SamplingParams","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:38:07.007416Z","iopub.execute_input":"2025-04-15T19:38:07.007769Z","iopub.status.idle":"2025-04-15T19:38:07.013842Z","shell.execute_reply.started":"2025-04-15T19:38:07.007735Z","shell.execute_reply":"2025-04-15T19:38:07.013025Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"if not os.path.exists(\"/kaggle/working/triviaqa-rc\"):\n    # Download the dataset in the working space of Kaggle\n    !wget https://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz\n    # Create a directory for extracting the content\n    !mkdir /kaggle/working/triviaqa-rc\n    # Extract the content from the downloaded file\n    !tar -xzf /kaggle/working/triviaqa-rc.tar.gz -C /kaggle/working/triviaqa-rc\n    # Delete the compressed file\n    !rm /kaggle/working/triviaqa-rc.tar.gz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:03:23.037506Z","iopub.execute_input":"2025-04-15T20:03:23.037871Z","iopub.status.idle":"2025-04-15T20:05:05.255701Z","shell.execute_reply.started":"2025-04-15T20:03:23.037843Z","shell.execute_reply":"2025-04-15T20:05:05.254555Z"}},"outputs":[{"name":"stdout","text":"--2025-04-15 20:03:23--  https://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz\nResolving nlp.cs.washington.edu (nlp.cs.washington.edu)... 128.208.3.117, 2607:4000:200:12:3eec:efff:fe5e:6f68\nConnecting to nlp.cs.washington.edu (nlp.cs.washington.edu)|128.208.3.117|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2665779500 (2.5G) [application/x-gzip]\nSaving to: ‘triviaqa-rc.tar.gz’\n\ntriviaqa-rc.tar.gz  100%[===================>]   2.48G   105MB/s    in 27s     \n\n2025-04-15 20:03:50 (93.1 MB/s) - ‘triviaqa-rc.tar.gz’ saved [2665779500/2665779500]\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Load the original training file\ntrain_data = pd.read_json(\"/kaggle/working/triviaqa-rc/qa/wikipedia-train.json\")\n\n# Split the data: first 7900 questions for validation, the rest for training\nvalidation_data = train_data.iloc[:7900]\ntrain_data = train_data.iloc[7900:] # We shouldn't use this (it's not necessary if we are using a RAG system)\n\n# Load the original validation file to use as test data\ntest_data = pd.read_json(\"/kaggle/working/triviaqa-rc/qa/wikipedia-dev.json\") # The actual test data is hidden\n\nprint(\"Data partitioned successfully:\")\nprint(f\"Training set size: {len(train_data)}\")\nprint(f\"Validation set size: {len(validation_data)}\")\nprint(f\"Test set size: {len(test_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:05:30.915774Z","iopub.execute_input":"2025-04-15T20:05:30.916126Z","iopub.status.idle":"2025-04-15T20:05:33.259669Z","shell.execute_reply.started":"2025-04-15T20:05:30.916101Z","shell.execute_reply":"2025-04-15T20:05:33.258690Z"}},"outputs":[{"name":"stdout","text":"Data partitioned successfully:\nTraining set size: 53988\nValidation set size: 7900\nTest set size: 7993\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"validation_data.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Build a list of docs from the downloaded Wikipedia documents\ndef getDocs(examples):\n    # List of docs\n    docs = []\n    # Extracting metadata and filenames from the examples (training data)\n    for example in examples[\"Data\"]:\n        filename = example[\"EntityPages\"][0][\"Filename\"]\n        with open(f\"triviaqa-rc/evidence/wikipedia/{filename}\", \"r\") as file:\n            context_text = file.read()\n        # Create a Document for RAG\n        newDoc = LangchainDocument(\n                metadata={\n                    'question_id': example['QuestionId'],\n                    'source': example['EntityPages'][0]['DocSource'],\n                    'answer_type': example['Answer']['Type'],\n                    'entity_name': example['Answer'].get('NormalizedValue', ''), # Default to empty string if missing\n                    'aliases': example['Answer'].get('Aliases', []), # Default to empty list if missing\n                    'normalized_value': example['Answer'].get('NormalizedValue', ''), # Default to empty string if missing\n                    'filename': example['EntityPages'][0]['Filename'],\n                },\n                page_content=context_text\n        )\n        docs.append(newDoc)\n    return docs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the validation data to decide the size of each document chunk\nRAW_KNOWLEDGE_BASE = getDocs(validation_data)\n\n# Use the test data to create the knowledge base\n#RAW_KNOWLEDGE_BASE = getDocs(test_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n# This list is taken from LangChain's MarkdownTextSplitter class\n# 在这里选择chunk大小\n'''\nMARKDOWN_SEPARATORS = [\n    \"\\n#{1,6} \",\n    \"```\\n\",\n    \"\\n\\\\*\\\\*\\\\*+\\n\",\n    \"\\n---+\\n\",\n    \"\\n___+\\n\",\n    \"\\n\\n\",\n    \"\\n\",\n    \" \",\n    \"\",\n]\n'''\nMARKDOWN_SEPARATORS = [\n    \"\\n\\n\",  # Paragraph breaks\n    \"\\n\",    # Line breaks\n    \". \",    # Sentences\n    \"? \",    # Questions\n    \"! \",    # Exclamations\n    \"; \",    # Semicolons\n    \": \",    # Colons\n    \", \",    # Commas\n    \" \",     # Words\n    \"\"       # Characters\n]\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, # The maximum number of characters in a chunk: we selected this value arbitrarily \n    chunk_overlap=100, # The number of characters to overlap between chunks\n    add_start_index=True, # If `True`, includes chunk's start index in metadata\n    strip_whitespace=True, # If `True`, strips whitespace from the start and end of every document\n    separators=MARKDOWN_SEPARATORS,\n)\n\ndocs_processed = []\nfor doc in RAW_KNOWLEDGE_BASE: # We are using the validation data to test\n    docs_processed += text_splitter.split_documents([doc])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\nprint(\n    f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\nlengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n\n# Plot the distribution of document lengths, counted as the number of tokens\nfig = pd.Series(lengths).hist()\nplt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EMBEDDING_MODEL_NAME = \"thenlper/gte-small\" # The name of the SentenceTransformer\n# gte-small和一个LLM最前面的一步差不多字，token，embedding取出tokenization\n# Function to split the documents into chunks\ndef split_documents(\n    chunk_size: int,\n    knowledge_base: List[LangchainDocument],\n    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n) -> List[LangchainDocument]:\n    \"\"\"\n    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n    \"\"\"\n\n    '''\n    MARKDOWN_SEPARATORS = [\n        \"\\n#{1,6} \",\n        \"```\\n\",\n        \"\\n\\\\*\\\\*\\\\*+\\n\",\n        \"\\n---+\\n\",\n        \"\\n___+\\n\",\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \"\",\n    ]\n    '''\n    MARKDOWN_SEPARATORS = [\n        \"\\n\\n\",  # Paragraph breaks\n        \"\\n\",    # Line breaks\n        \". \",    # Sentences\n        \"? \",    # Questions\n        \"! \",    # Exclamations\n        \"; \",    # Semicolons\n        \": \",    # Colons\n        \", \",    # Commas\n        \" \",     # Words\n        \"\"       # Characters\n    ]\n\n    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        AutoTokenizer.from_pretrained(tokenizer_name),\n        chunk_size=chunk_size,\n        chunk_overlap=int(chunk_size / 10),\n        add_start_index=True,\n        strip_whitespace=True,\n        separators=MARKDOWN_SEPARATORS,\n    )\n\n    docs_processed = []\n    for doc in knowledge_base:\n        docs_processed += text_splitter.split_documents([doc])\n\n    # Remove duplicates\n    unique_texts = {}\n    docs_processed_unique = []\n    for doc in docs_processed:\n        if doc.page_content not in unique_texts:\n            unique_texts[doc.page_content] = True\n            docs_processed_unique.append(doc)\n\n    return docs_processed_unique\n\n# Processing the RAW_KNOWLEDGE_BASE (validation_data)\ndocs_processed = split_documents(\n    512, # We choose a chunk size adapted to our model\n    RAW_KNOWLEDGE_BASE,\n    tokenizer_name=EMBEDDING_MODEL_NAME,\n)\n\n# Save the object\nwith open(\"/kaggle/working/docs_processed.pkl\", \"wb\") as file:\n    pickle.dump(docs_processed, file)\n\ntokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\nlengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\nfig = pd.Series(lengths).hist()\nplt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The name of the embedding model\nEMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n# HuggingFace Embedding Model\nembedding_model = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    multi_process=True,\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:38:57.699703Z","iopub.execute_input":"2025-04-15T19:38:57.700088Z","iopub.status.idle":"2025-04-15T19:39:04.003867Z","shell.execute_reply.started":"2025-04-15T19:38:57.700056Z","shell.execute_reply":"2025-04-15T19:39:04.003062Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"403521aad0ce4ca39b1cad3978bbab8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af5ebed37ec94f54be83f25cf705fc65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"569b425c2f224af79002d2fc6d754d80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d63d4f5dc556417db22354de1bb21c9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ca7df3e135b4ac6826754d5633b07de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a4c11427764b4f8a149f513ffd52ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"858ac85182a946cb8c2835211868ae90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6367b1af4bab4f3889e45aa6e1c1c0e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0678ce87c86240989823d281a0b9252d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efc5d288abf84a979c1dd923fb5ae2df"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# This cell below takes around half an hour to run\n# 不用跑 \n# # Create the vector database of document embeddings\nKNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n)\n\n# #Save the vector database in a file\nfaiss_index_path = \"/kaggle/working/knowledge_vector_database-validation\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 加载上传的本地数据集要跑\n# Load the pre-created vector database\nfaiss_index_path = \"/kaggle/input/dataset/knowledge_vector_database-validation\"\n# faiss_index_path = \"/kaggle/working/knowledge-vector-database-validation\"\nKNOWLEDGE_VECTOR_DATABASE = FAISS.load_local(faiss_index_path, embedding_model, allow_dangerous_deserialization=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# KNOWLEDGE_VECTOR_DATABASE.save_local(faiss_index_path)\n# 这个好像也不用跑","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Embed a user query in the same space\nuser_query = \"Which Mediterranean island was once known as Alashiya?\"\nquery_vector = embedding_model.embed_query(user_query)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_projector = pacmap.PaCMAP(\n    n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1\n)\n\nembeddings_2d = [\n    list(KNOWLEDGE_VECTOR_DATABASE.index.reconstruct_n(idx, 1)[0])\n    for idx in range(len(docs_processed))\n] + [query_vector]\n\n# Fit the data (the index of transformed data corresponds to the index of the original data)\ndocuments_projected = embedding_projector.fit_transform(\n    np.array(embeddings_2d), init=\"pca\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pio.renderers.default = 'iframe'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataFrame for document embeddings \ndf = pd.DataFrame.from_dict(\n    [\n        {\n            \"x\": documents_projected[i, 0],  # Accessing document projection\n            \"y\": documents_projected[i, 1],\n            \"source\": docs_processed[i].metadata[\"source\"],\n            \"extract\": docs_processed[i].page_content[:100] + \"...\",\n            \"symbol\": \"circle\",\n            \"size_col\": 4,\n        }\n        for i in range(len(docs_processed))  # Only iterate over document embeddings\n    ]\n    + [\n        {\n            \"x\": documents_projected[-1, 0],  # Append the query vector\n            \"y\": documents_projected[-1, 1],\n            \"source\": \"User query\",\n            \"extract\": user_query,\n            \"size_col\": 100,\n            \"symbol\": \"star\",\n        }\n    ]\n)\n\n# Visualize the embedding\nfig = px.scatter(\n    df,\n    x=\"x\",\n    y=\"y\",\n    color=\"source\",\n    text=\"extract\",\n    symbol=\"symbol\",\n    size=\"size_col\",\n    title=\"2D Projection of Documents and Query\",\n    width=1000,\n    height=700,\n)\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"\\nStarting retrieval for {user_query=}...\")\nretrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\nprint(\n    \"\\n==================================Top 1 document==================================\"\n)\nprint(retrieved_docs[0].page_content)\nprint(\"==================================Metadata==================================\")\nprint(retrieved_docs[0].metadata)\nprint(\n    \"\\n==================================Top 2 document==================================\"\n)\nprint(retrieved_docs[1].page_content)\nprint(\"==================================Metadata==================================\")\nprint(retrieved_docs[1].metadata)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rank_bm25 import BM25Okapi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:40:04.065086Z","iopub.execute_input":"2025-04-15T19:40:04.065459Z","iopub.status.idle":"2025-04-15T19:40:04.070325Z","shell.execute_reply.started":"2025-04-15T19:40:04.065426Z","shell.execute_reply":"2025-04-15T19:40:04.069542Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Extract the page content from the documents\ndoc_texts = [doc.page_content for doc in docs_processed]\n\n# Tokenize the document texts\ntokenized_corpus = [text.split() for text in doc_texts]\n\n# Initialize BM25 retriever\nbm25 = BM25Okapi(tokenized_corpus)\n\n# File path for saving the BM25 retriever\nbm25_file_path = '/kaggle/working/bm25_retriever_validation.pkl'\n\n# Save the tokenized corpus\nwith open(bm25_file_path, 'wb') as f:\n    pickle.dump(tokenized_corpus, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instead\n# 这个不跑\n# Load the object\nwith open(\"/kaggle/input/docs-processed-validation/docs_processed.pkl\", \"rb\") as file:\n    docs_processed = pickle.load(file)\n\n# Load the tokenized corpus\nwith open('/kaggle/input/bm25-retriever-validation/bm25_retriever_validation.pkl', 'rb') as f:\n    loaded_tokenized_corpus = pickle.load(f)\n\n# Reinitialize BM25 retriever using the loaded corpus\nbm25 = BM25Okapi(loaded_tokenized_corpus)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bm25_retrieve(query, k=5):\n    \"\"\"\n    Perform retrieval using BM25 on LangChain documents.\n\n    Args:\n        query (str): The search query.\n        k (int): The number of top documents to retrieve.\n\n    Returns:\n        list: A list of tuples with document indices, scores, and the documents themselves.\n    \"\"\"\n    # Tokenize the query\n    tokenized_query = query.split()\n\n    # Get scores for all documents\n    scores = bm25.get_scores(tokenized_query)\n\n    # Get top-k document indices and scores\n    top_k_indices = np.argsort(scores)[::-1][:k]\n    top_k_results = [(index, scores[index], docs_processed[index]) for index in top_k_indices]\n\n    return top_k_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:40:12.979026Z","iopub.execute_input":"2025-04-15T19:40:12.979338Z","iopub.status.idle":"2025-04-15T19:40:12.984458Z","shell.execute_reply.started":"2025-04-15T19:40:12.979316Z","shell.execute_reply":"2025-04-15T19:40:12.983546Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Example query\nquery = \"Which Lloyd Webber musical premiered in the US on 10th December 1993?\"\ntop_docs = bm25_retrieve(query, k=3)\n\n# Display results\nfor idx, score, doc in top_docs:\n    print(\"================================================================================================\")\n    print(f\"Doc Index: {idx}, Score: {score}, Content: {doc.page_content}, Metadata: {doc.metadata}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install together","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:40:16.903904Z","iopub.execute_input":"2025-04-15T19:40:16.904230Z","iopub.status.idle":"2025-04-15T19:40:22.053730Z","shell.execute_reply.started":"2025-04-15T19:40:16.904205Z","shell.execute_reply":"2025-04-15T19:40:22.052528Z"}},"outputs":[{"name":"stdout","text":"Collecting together\n  Downloading together-1.5.5-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /usr/local/lib/python3.10/dist-packages (from together) (3.11.12)\nRequirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from together) (8.1.7)\nRequirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from together) (0.2.0)\nRequirement already satisfied: filelock<4.0.0,>=3.13.1 in /usr/local/lib/python3.10/dist-packages (from together) (3.17.0)\nRequirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from together) (1.26.4)\nCollecting pillow<12.0.0,>=11.1.0 (from together)\n  Downloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\nRequirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from together) (19.0.1)\nRequirement already satisfied: pydantic<3.0.0,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from together) (2.11.0a2)\nRequirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from together) (2.32.3)\nRequirement already satisfied: rich<14.0.0,>=13.8.1 in /usr/local/lib/python3.10/dist-packages (from together) (13.9.4)\nRequirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from together) (0.9.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.2 in /usr/local/lib/python3.10/dist-packages (from together) (4.67.1)\nRequirement already satisfied: typer<0.16,>=0.9 in /usr/local/lib/python3.10/dist-packages (from together) (0.15.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->together) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->together) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->together) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->together) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->together) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->together) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.8.1->together) (2.19.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.16,>=0.9->together) (1.5.4)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.23.5->together) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.23.5->together) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.5->together) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.23.5->together) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.23.5->together) (2024.2.0)\nDownloading together-1.5.5-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pillow, together\n  Attempting uninstall: pillow\n    Found existing installation: pillow 11.0.0\n    Uninstalling pillow-11.0.0:\n      Successfully uninstalled pillow-11.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pillow-11.2.1 together-1.5.5\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from together import Together\nimport tiktoken\nimport numpy as np\nfrom typing import List\n\n# 初始化 Together API 客户端\nclient = Together(api_key=\"API_KEY\")\n\n# 获取 Llama-3.1-8B-Instruct-Turbo 的 tokenizer\nenc = tiktoken.get_encoding(\"cl100k_base\")\n\n# Sampling 参数类\nclass SamplingParams:\n    def __init__(self, n=1, top_p=0.9, temperature=0.7, repetition_penalty=1.2, max_tokens=256):\n        self.n = n\n        self.top_p = top_p\n        self.temperature = temperature\n        self.repetition_penalty = repetition_penalty\n        self.max_tokens = max_tokens\n\n# 自定义 Tokenizer 代理类\nclass TogetherTokenizer:\n    \"\"\" 模拟 transformers.Tokenizer，提供 `apply_chat_template` 和 `tokenize` 以兼容原代码 \"\"\"\n\n    def __init__(self):\n        pass\n\n    def apply_chat_template(self, messages, tokenize=False, add_generation_prompt=True):\n        \"\"\" 模拟 `apply_chat_template`，将聊天格式转换为文本 \"\"\"\n        chat_template = \"\"\n        for msg in messages:\n            role = msg.get(\"role\", \"user\")\n            content = msg.get(\"content\", \"\")\n            chat_template += f\"{role.capitalize()}: {content}\\n\"\n\n        if add_generation_prompt:\n            chat_template += \"Assistant: \"  # 让 LLM 继续生成回答\n        \n        return chat_template if not tokenize else chat_template.split()\n\n    def tokenize(self, text):\n        \"\"\" 使用 `tiktoken` 进行 tokenization \"\"\"\n        return enc.encode(text)\n\n# Together LLM Reader\nclass TogetherLLMReader:\n    def __init__(self, model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\", api_key=\"API_KEY\"):\n        self.client = Together(api_key=api_key)\n        self.model_name = model_name\n        self.tokenizer = TogetherTokenizer()\n\n    def get_tokenizer(self):\n        \"\"\" 兼容原代码，返回自定义 tokenizer \"\"\"\n        return self.tokenizer\n\n    def generate(self, prompts: List[str], sampling_params: SamplingParams):\n        \"\"\" 兼容原代码，支持批量 Prompt 生成 \"\"\"\n        responses = []\n        for prompt in prompts:\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=sampling_params.temperature,\n                max_tokens=sampling_params.max_tokens,\n                top_p=sampling_params.top_p,\n                repetition_penalty=sampling_params.repetition_penalty,\n                n=sampling_params.n,\n            )\n            responses.append(response.choices[0].message.content)\n\n        # 兼容原代码：包装返回结果\n        return [LLMOutput(prompt, generated_text) for prompt, generated_text in zip(prompts, responses)]\n\n# 兼容原代码的输出格式\nclass LLMOutput:\n    def __init__(self, prompt, text):\n        self.prompt = prompt\n        self.outputs = [LLMResponse(text)]\n\nclass LLMResponse:\n    def __init__(self, text):\n        self.text = text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:40:22.880754Z","iopub.execute_input":"2025-04-15T19:40:22.881076Z","iopub.status.idle":"2025-04-15T19:40:23.884498Z","shell.execute_reply.started":"2025-04-15T19:40:22.881050Z","shell.execute_reply":"2025-04-15T19:40:23.883772Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n# Define the model\n# from together import Together\n'''\nREADER_MODEL_NAME = \"/kaggle/input/zephyr-7b-beta-awq\"\n#READER_MODEL_NAME = \"/kaggle/input/hugging-quants-meta-llama-3-1-8b-instruct-awq-int4\"\n#READER_MODEL_NAME = \"PyrTools/Ministral-8B-Instruct-2410-AWQ\"\n\n# Configure the model\n\nmodel = LLM( \n    model = READER_MODEL_NAME,\n    quantization=\"awq\",\n    tensor_parallel_size=2, \n    gpu_memory_utilization=0.95, \n    trust_remote_code=True,\n    dtype=\"half\", \n    enforce_eager=True,\n    #max_model_len=1024,\n    disable_log_stats=True\n)\n\ntokenizer = model.get_tokenizer()\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 初始化模型\nmodel = TogetherLLMReader(api_key=\"API_KEY\")\n\n# 获取 tokenizer\ntokenizer = model.get_tokenizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:40:27.695109Z","iopub.execute_input":"2025-04-15T19:40:27.695504Z","iopub.status.idle":"2025-04-15T19:40:27.699736Z","shell.execute_reply.started":"2025-04-15T19:40:27.695450Z","shell.execute_reply":"2025-04-15T19:40:27.698754Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Initialize a variable to accumulate the average token counts\nacmul = 0\n\nfor instance in validation_data.Data.iloc:\n  # Check for the existence of the Value field and other fields; default to an empty list if missing\n  value = instance['Answer'].get('Value', '') # Use an empty string if 'Value' is missing\n  normalized_value = instance['Answer'].get('NormalizedValue', '') # Use an empty string if 'NormalizedValue' is missing\n  aliases = instance['Answer'].get('Aliases', []) # Use an empty list if 'Aliases' is missing\n  normalized_aliases = instance['Answer'].get('NormalizedAliases', []) # Use an empty list if 'NormalizedAliases' is missing\n\n  # Combine all available fields into one list of strings\n  all_text = [value, normalized_value] + aliases + normalized_aliases\n  # Tokenize each text and calculate token counts\n  token_counts = [len(tokenizer.tokenize(text)) for text in all_text]\n  # Compute average token count\n  average_tokens = np.mean(token_counts)\n  # Add the average token count to the accumulator\n  acmul += average_tokens\n\n# Calculate the overall average token count across all instances in the dataset\nfinal_average_tokens = acmul / len(validation_data)\n\nprint(f\"The average number of tokens for an answer is: {final_average_tokens:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the sampling parameters\nsampling_params = SamplingParams(\n    n = 1,\n    top_p=0.9,\n    temperature=0,\n    repetition_penalty=1.2,\n    max_tokens=5,     # Maximum number of tokens\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:10:50.067003Z","iopub.execute_input":"2025-04-15T20:10:50.067324Z","iopub.status.idle":"2025-04-15T20:10:50.071236Z","shell.execute_reply.started":"2025-04-15T20:10:50.067299Z","shell.execute_reply":"2025-04-15T20:10:50.070560Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"prompt_in_chat_format = [\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"Answer the question with only one word or the simplest possible response (e.g., a single number or a single word).\nDo NOT generate sentences, explanations, or additional context.\nStop immediately after providing the answer. Do not generate any further words or tokens.\nIf the context does not provide any useful information, answer the question based on your own knowledge.\nI am going to provide you five examples:\n\nQuestion: What is the capital of Kenya?\nAnswer: Nairobi\n---\nQuestion: What was the name of the pig leader in George Orwell's Animal Farm?\nAnswer: Napoleon\n---\nQuestion: Which artist created the Katzenjammer Kids?\nAnswer: Rudolph Dirks\n---\nQuestion: Who was Geena Davis's husband when they made the loss-maker Cutthroat Island?\nAnswer: Renny Harlin\n---\nQuestion: Who was married to Spandau Ballet's Gary Kemp and later to Jude Law?\nAnswer: Sadie Frost\n\n\"\"\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"Context:\n{context}\n---\nNow here is the question you need to answer.\n\nQuestion: {question}\"\"\"\n    },\n]\n\nRAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n)\nprint(RAG_PROMPT_TEMPLATE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:40:32.665342Z","iopub.execute_input":"2025-04-15T19:40:32.665738Z","iopub.status.idle":"2025-04-15T19:40:32.671081Z","shell.execute_reply.started":"2025-04-15T19:40:32.665706Z","shell.execute_reply":"2025-04-15T19:40:32.670127Z"}},"outputs":[{"name":"stdout","text":"System: Answer the question with only one word or the simplest possible response (e.g., a single number or a single word).\nDo NOT generate sentences, explanations, or additional context.\nStop immediately after providing the answer. Do not generate any further words or tokens.\nIf the context does not provide any useful information, answer the question based on your own knowledge.\nI am going to provide you five examples:\n\nQuestion: What is the capital of Kenya?\nAnswer: Nairobi\n---\nQuestion: What was the name of the pig leader in George Orwell's Animal Farm?\nAnswer: Napoleon\n---\nQuestion: Which artist created the Katzenjammer Kids?\nAnswer: Rudolph Dirks\n---\nQuestion: Who was Geena Davis's husband when they made the loss-maker Cutthroat Island?\nAnswer: Renny Harlin\n---\nQuestion: Who was married to Spandau Ballet's Gary Kemp and later to Jude Law?\nAnswer: Sadie Frost\n\n\nUser: Context:\n{context}\n---\nNow here is the question you need to answer.\n\nQuestion: {question}\nAssistant: \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"question = \"Which Lloyd Webber musical premiered in the US on 10th December 1993?\"\ncontext = \"\"\n\n\nprompt = [RAG_PROMPT_TEMPLATE.format(question = question, context = context)]\noutputs = model.generate(prompt, sampling_params)\n\nfor output in outputs:\n    generated_text = output.outputs[0].text\n    prompt = output.prompt\n    print(f\"Question: {question!r}\")\n    print(f\"Prompt: {prompt!r}\")\n    print(f\"Generated text: {generated_text!r}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Context provided by the embedding model\n\nquestion = \"Which Lloyd Webber musical premiered in the US on 10th December 1993?\"\n\nrelevant_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(question, k=3)\nrelevant_docs = [doc.page_content for doc in relevant_docs]\ncontext = \"\\nExtracted documents:\\n\"\ncontext += \"\".join(\n            [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n        )\n\nprompt = [RAG_PROMPT_TEMPLATE.format(question = question, context = context)]\noutputs = model.generate(prompt, sampling_params)\n\nfor output in outputs:\n    generated_text = output.outputs[0].text\n    prompt = output.prompt\n    print(f\"Question: {question!r}\")\n    print(f\"Prompt: {prompt!r}\")\n    print(f\"Generated text: {generated_text!r}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from ragatouille import RAGPretrainedModel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main function to answer questions using RAG\ndef answer_with_rag(\n    questions: List[str],\n    llm,\n    prompt_template,\n    sampling_params: dict,\n    knowledge_index=None,\n    embedding_model=None,\n    reranker = None,\n    # reranker: Optional[RAGPretrainedModel] = None,\n    num_retrieved_docs: int = 30,\n    num_docs_final: int = 3,\n    printing: bool = True,\n    retriever: str = \"faiss\"\n) -> List[str]:\n    \"\"\"\n    Main function for answering questions using a Retrieval-Augmented Generation (RAG) pipeline.\n    \"\"\"\n    # Ensure that questions are in list format\n    if isinstance(questions, str):\n        questions = [questions]\n\n    # Step 1: Generate embeddings for all questions in one go if using 'faiss'\n    if retriever == \"faiss\":\n        if embedding_model is None or knowledge_index is None:\n            raise ValueError(\"For 'faiss' retriever, 'embedding_model' and 'knowledge_index' must be provided.\")\n        \n        # Calculate all embeddings for the questions at once\n        embeddings = embedding_model.embed_documents(questions)\n\n    # Step 2: Retrieve contexts for each question\n    contexts = []\n    relevant_docs_list = []\n    \n    for idx, question in enumerate(questions):\n        # Use the precomputed embedding for each question if using 'faiss'\n        embedding = embeddings[idx] if retriever == \"faiss\" else None\n        context, relevant_docs = retrieve(\n            question,\n            embedding=embedding,\n            knowledge_index=knowledge_index if retriever == \"faiss\" else None,\n            reranker=reranker,\n            num_retrieved_docs=num_retrieved_docs,\n            num_docs_final=num_docs_final,\n            printing=printing,\n            retriever=retriever,\n        )\n        contexts.append(context)\n        relevant_docs_list.append(relevant_docs)\n\n    # Step 3: Generate answers using the LLM model\n    if printing:\n        print(\"=> Generating answers...\")\n    answers = read(llm, sampling_params, prompt_template, contexts, questions)\n\n    return answers, relevant_docs_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:40:40.161089Z","iopub.execute_input":"2025-04-15T19:40:40.161456Z","iopub.status.idle":"2025-04-15T19:40:40.168154Z","shell.execute_reply.started":"2025-04-15T19:40:40.161426Z","shell.execute_reply":"2025-04-15T19:40:40.167160Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def silent_rerank(reranker, question, relevant_docs, k):\n    # Create a temporary buffer to capture stdout and stderr\n    with io.StringIO() as buf, contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n        # Call the rerank function\n        ranked_docs = reranker.rerank(question, relevant_docs, k=k)\n    return ranked_docs\n\ndef retrieve(\n    question: str,\n    embedding: Optional[List[float]] = None,\n    knowledge_index=None,\n    reranker = None,\n    # reranker: Optional[RAGPretrainedModel] = None,\n    num_retrieved_docs: int = 30,\n    num_docs_final: int = 3,\n    printing: bool = True,\n    retriever: str = \"faiss\"\n) -> Tuple[str, List[str]]:\n    \"\"\"\n    Retrieves and optionally reranks documents from the knowledge index.\n    \"\"\"\n    if retriever not in {\"faiss\", \"bm25\", None}:\n        raise ValueError(f\"Unsupported retriever: {retriever}\")\n\n    # Step 1: Retrieve initial documents if a retriever is used\n    if retriever == \"faiss\" or retriever == \"bm25\":\n        if printing:\n            print(\"=> Retrieving documents...\")\n\n        relevant_docs = []\n\n        if retriever == \"faiss\":\n            if embedding is None or knowledge_index is None:\n                raise ValueError(\"For 'faiss' retriever, 'embedding' and 'knowledge_index' must be provided.\")\n            # Perform search using the precomputed embedding\n            relevant_docs = knowledge_index.similarity_search_by_vector(embedding, k=num_retrieved_docs)\n            relevant_docs = [doc.page_content for doc in relevant_docs]  # Extract only the content\n        elif retriever == \"bm25\":\n            # Retrieve documents using BM25\n            retrieved_docs = bm25_retrieve(question, k=num_retrieved_docs)\n            relevant_docs = [doc.page_content for _, _, doc in retrieved_docs]\n\n        # Step 2: Optionally rerank the retrieved documents\n        if reranker:\n            if printing:\n                print(\"=> Reranking documents...\")\n            relevant_docs = silent_rerank(reranker, question, relevant_docs, k=num_docs_final)\n            relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n\n        # Limit the number of final documents to the desired count\n        relevant_docs = relevant_docs[:num_docs_final]\n\n        # Step 3: Build the context for the LLM model\n        context = \"\\nExtracted documents:\\n\" + \"\".join(\n            [f\"Document {i}:::\\n{doc}\\n\" for i, doc in enumerate(relevant_docs)]\n        )\n    else:\n        # If no retriever is provided, set the context as empty\n        context = \"\"\n        relevant_docs = [\"\"]\n\n    return context, relevant_docs\n\ndef read(llm, sampling_params, prompt_template, contexts, questions):\n    \"\"\"\n    Generates answers from the LLM by formatting the question-context pairs into prompts.\n    \"\"\"\n    # Format prompts by combining questions and contexts\n    prompts = [prompt_template.format(question=q, context=c) for q, c in zip(questions, contexts)]\n    \n    # Generate answers using the LLM\n    outputs = llm.generate(prompts, sampling_params)\n    \n    # Extract the generated text from the outputs\n    outputs = [output.outputs[0].text for output in outputs]\n    \n    return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:40:46.297892Z","iopub.execute_input":"2025-04-15T19:40:46.298281Z","iopub.status.idle":"2025-04-15T19:40:46.307539Z","shell.execute_reply.started":"2025-04-15T19:40:46.298250Z","shell.execute_reply":"2025-04-15T19:40:46.306605Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"validation_data_batch = validation_data[0:10] \nquestions = [instance[\"Question\"] for instance in validation_data_batch[\"Data\"]]\n\nanswers, _ = answer_with_rag(\n    questions= questions,\n    llm = model,\n    prompt_template = RAG_PROMPT_TEMPLATE,\n    sampling_params = sampling_params,\n    knowledge_index= None,\n    embedding_model= None,\n    reranker = None,\n    # reranker = RERANKER,\n    num_retrieved_docs = 30,\n    num_docs_final = 3,\n    printing = True,\n    retriever = \"bm25\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for q,a in zip(questions, answers):\n    print(\"Question: \", q)\n    print(\"Answer: \", a)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question = \"Where was born the Queen Elizabeth II?\"\n\nanswers, relevant_docs_list = answer_with_rag(\n    questions= question,\n    llm = model,\n    prompt_template = RAG_PROMPT_TEMPLATE,\n    sampling_params = sampling_params,\n    knowledge_index= KNOWLEDGE_VECTOR_DATABASE, \n    embedding_model= embedding_model,\n    reranker = None, # If you don't want to use the reranker, set reranker = None\n    num_retrieved_docs = 30,\n    num_docs_final = 3,\n    printing = True,\n    retriever = \"bm25\" # Retriever options: \"bm25\", \"faiss\", None\n)\n\nprint(\"Question: \", question)\nprint(\"Answer: \", answers[0])\nprint(\"Relevant Docs: \", relevant_docs_list[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_data_batch = validation_data[0:10] \nquestions = [instance[\"Question\"] for instance in validation_data_batch[\"Data\"]]\n\nanswers, _ = answer_with_rag(\n    questions= questions,\n    llm = model,\n    prompt_template = RAG_PROMPT_TEMPLATE,\n    sampling_params = sampling_params,\n    knowledge_index= KNOWLEDGE_VECTOR_DATABASE, \n    embedding_model= embedding_model,\n    reranker = None, # If you don't want to use the reranker, set reranker = None\n    num_retrieved_docs = 30,\n    num_docs_final = 3,\n    printing = False,\n    retriever = \"faiss\" # Retriever options: \"bm25\", \"faiss\", None\n)\n\nfor q,a in zip(questions, answers):\n    print(\"Question: \", q)\n    print(\"Answer: \", a)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/mandarjoshi90/triviaqa.git\n!pip install -q -r /kaggle/working/triviaqa/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:40:54.425730Z","iopub.execute_input":"2025-04-15T19:40:54.426105Z","iopub.status.idle":"2025-04-15T19:40:58.877199Z","shell.execute_reply.started":"2025-04-15T19:40:54.426069Z","shell.execute_reply":"2025-04-15T19:40:58.875900Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'triviaqa'...\nremote: Enumerating objects: 70, done.\u001b[K\nremote: Total 70 (delta 0), reused 0 (delta 0), pack-reused 70 (from 1)\u001b[K\nReceiving objects: 100% (70/70), 20.60 KiB | 5.15 MiB/s, done.\nResolving deltas: 100% (28/28), done.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def perform_inference_p(instances, retriever, embedding_model=None, knowledge_index=None):\n    # Processes a batch of instances to generate answers\n    questions = [instance[\"Question\"] for instance in instances]\n    question_ids = [instance[\"QuestionId\"] for instance in instances]\n\n    # Generates answers using `answer_with_rag`\n    '''\n    answer_with_rag( \n    questions: List[str],\n    llm,\n    prompt_template,\n    sampling_params: dict,\n    knowledge_index=None,\n    embedding_model=None,\n    reranker = None,\n    # reranker: Optional[RAGPretrainedModel] = None,\n    num_retrieved_docs: int = 30,\n    num_docs_final: int = 3,\n    printing: bool = True,\n    retriever: str = \"faiss\"\n)\n    '''\n    responses, _ = answer_with_rag(\n        questions= questions,\n        llm = model,\n        prompt_template = RAG_PROMPT_TEMPLATE,\n        sampling_params = sampling_params,\n        knowledge_index=knowledge_index,\n        embedding_model=embedding_model,\n        reranker=None,\n        printing=False,\n        retriever=retriever,\n    )\n\n    torch.cuda.empty_cache()\n\n    # Associates the answers with their respective IDs\n    results = [{\"QuestionId\": qid, \"Answer\": answer} for qid, answer in zip(question_ids, responses)]\n    return results\n\ndef parallel_inference(validation_data, retriever=\"None\", embedding_model=None, knowledge_index=None):\n    # Initialize structures for predictions and TriviaQA data\n    predictions = {}\n    triviaqa_instances = {\n        \"Data\": [],\n        \"Domain\": \"Wikipedia\",\n        \"VerifiedEval\": False,\n        \"Version\": 1.0,\n    }\n\n    # Call perform_inference_p\n    results = perform_inference_p(validation_data[\"Data\"], retriever, embedding_model, knowledge_index)\n\n    # Store the predictions\n    for result in results:\n        question_id = result[\"QuestionId\"]\n        answer = result[\"Answer\"]\n        predictions[question_id] = answer\n\n    # Add the original instances to the TriviaQA set\n    triviaqa_instances[\"Data\"].extend(validation_data[\"Data\"])\n\n    return predictions, triviaqa_instances","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:41:10.472206Z","iopub.execute_input":"2025-04-15T19:41:10.472560Z","iopub.status.idle":"2025-04-15T19:41:10.479237Z","shell.execute_reply.started":"2025-04-15T19:41:10.472532Z","shell.execute_reply":"2025-04-15T19:41:10.478320Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Example from the repository\n!cd /kaggle/working/triviaqa && python3 -m evaluation.triviaqa_evaluation --dataset_file /kaggle/working/triviaqa/samples/triviaqa_sample.json --prediction_file /kaggle/working/triviaqa/samples/sample_predictions.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:41:17.300193Z","iopub.execute_input":"2025-04-15T19:41:17.300575Z","iopub.status.idle":"2025-04-15T19:41:17.558984Z","shell.execute_reply.started":"2025-04-15T19:41:17.300540Z","shell.execute_reply":"2025-04-15T19:41:17.558025Z"}},"outputs":[{"name":"stdout","text":"Missed question tc_33--35/35_995.txt will receive score 0.\n{'exact_match': 50.0, 'f1': 50.0, 'common': 1, 'denominator': 2, 'pred_len': 1, 'gold_len': 2}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# 已经传进input了不用手动生成，不用跑这个\nRAW_KNOWLEDGE_BASE_TEST = getDocs(test_data)\n\n# Processing the RAW_KNOWLEDGE_BASE_TEST (test_data)\ndocs_processed_test = split_documents(\n    512, # We choose a chunk size adapted to our model\n    RAW_KNOWLEDGE_BASE_TEST,\n    tokenizer_name=EMBEDDING_MODEL_NAME,\n)\n\n# Save the object\n'''\nwith open(\"/kaggle/working/docs_processed_test.pkl\", \"wb\") as file:\n    pickle.dump(docs_processed_test, file)\n\n# Create the vector database of document embeddings\nKNOWLEDGE_VECTOR_DATABASE_TEST = FAISS.from_documents(\n   docs_processed_test, embedding_model, distance_strategy=DistanceStrategy.COSINE\n)\n'''\n#Save the vector database in a file\nfaiss_index_path_test = \"/kaggle/working/knowledge_vector_database-test\"\nKNOWLEDGE_VECTOR_DATABASE_TEST.save_local(faiss_index_path_test)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the pre-created vector database\n# 改变数据库\n# faiss_index_path = \"/kaggle/input/faiss-500/Faiss\"\n# KNOWLEDGE_VECTOR_DATABASE_TEST = FAISS.load_local(faiss_index_path, embedding_model, allow_dangerous_deserialization=True)\nfaiss_index_path = \"/kaggle/input/faiss-500/Faiss\"\nKNOWLEDGE_VECTOR_DATABASE_TEST = FAISS.load_local(faiss_index_path, embedding_model, allow_dangerous_deserialization=True)\n\n# Load the test documents processed\nwith open(\"/kaggle/input/testset/pre-generated files/test_docs_processed.pkl\", \"rb\") as file:\n    docs_processed = pickle.load(file)\n\n# Load the tokenized corpus\n#with open('/kaggle/input/bm25-retriever/bm25_retriever.pkl', 'rb') as f:\n#    loaded_tokenized_corpus = pickle.load(f)\n\n# Reinitialize BM25 retriever using the loaded corpus\n# bm25 = BM25Okapi(loaded_tokenized_corpus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:43:23.900239Z","iopub.execute_input":"2025-04-15T19:43:23.900690Z","iopub.status.idle":"2025-04-15T19:43:29.903089Z","shell.execute_reply.started":"2025-04-15T19:43:23.900648Z","shell.execute_reply":"2025-04-15T19:43:29.901979Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"#print(f\"\\nStarting retrieval for {user_query=}...\")\n#retrieved_docs = KNOWLEDGE_VECTOR_DATABASE_TEST.similarity_search(query=user_query, k=5)\n#print(\n#    \"\\n==================================Top 1 document==================================\"\n#)\n#print(retrieved_docs[0].page_content)\n#print(\"==================================Metadata==================================\")\n#print(retrieved_docs[0].metadata)\n#print(\n#    \"\\n==================================Top 2 document==================================\"\n#)\n#print(retrieved_docs[1].page_content)\n#print(\"==================================Metadata==================================\")\n#print(retrieved_docs[1].metadata)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 定义文件夹路径\nfolder_path = '/kaggle/working/result_500'\n \n# 创建文件夹\nif not os.path.exists(folder_path):\n    os.makedirs(folder_path)\n    print(f\"Folder '{folder_path}' created successfully.\")\nelse:\n    print(f\"Folder '{folder_path}' already exists.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:42:21.629299Z","iopub.execute_input":"2025-04-15T19:42:21.629743Z","iopub.status.idle":"2025-04-15T19:42:21.635647Z","shell.execute_reply.started":"2025-04-15T19:42:21.629711Z","shell.execute_reply":"2025-04-15T19:42:21.634680Z"}},"outputs":[{"name":"stdout","text":"Folder '/kaggle/working/result_500' created successfully.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"batch_size = 100\nfor i in range(0, len(test_data), batch_size):\n\n    data = test_data.iloc[i:i+batch_size]\n    predictions, triviaqa_instances = parallel_inference(data, retriever=\"faiss\", embedding_model=embedding_model,\n        knowledge_index=KNOWLEDGE_VECTOR_DATABASE_TEST)\n    file_path_instances = f'/kaggle/working/result_500/zephyr_faiss_triviaqa_instances_{i+batch_size}.json'\n    file_path_predictions = f'/kaggle/working/result_500/zephyr_faiss_triviaqa_predictions_{i+batch_size}.json'\n  \n# Save the list of instances as s JSON file \n    with open(file_path_instances, 'w') as f:\n        json.dump(triviaqa_instances, f, indent=4)\n# Save the list of predictions as s JSON file\n    with open(file_path_predictions , 'w') as f:\n        json.dump(predictions, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:10:55.929028Z","iopub.execute_input":"2025-04-15T20:10:55.929347Z","iopub.status.idle":"2025-04-15T20:21:43.590940Z","shell.execute_reply.started":"2025-04-15T20:10:55.929321Z","shell.execute_reply":"2025-04-15T20:21:43.589355Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9b7f01aff974f67a5675dd7d715f005"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39e9519f93ed4a9eb55202212ed7a984"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728ae08e9a3744579554ac5a7398d5e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c42f54b7bf845998fb74d7a49b2133c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a1d66ae8a08489e9d3ffb82fa56f56e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6203878b9efa4dad94c064a50d48c830"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-b3d30a7bcf39>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     predictions, triviaqa_instances = parallel_inference(data, retriever=\"faiss\", embedding_model=embedding_model,\n\u001b[0m\u001b[1;32m      6\u001b[0m         knowledge_index=KNOWLEDGE_VECTOR_DATABASE_TEST)\n\u001b[1;32m      7\u001b[0m     \u001b[0mfile_path_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/kaggle/working/result_500/zephyr_faiss_triviaqa_instances_{i+batch_size}.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-c850b6b16fe4>\u001b[0m in \u001b[0;36mparallel_inference\u001b[0;34m(validation_data, retriever, embedding_model, knowledge_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Call perform_inference_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_inference_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknowledge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Store the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-c850b6b16fe4>\u001b[0m in \u001b[0;36mperform_inference_p\u001b[0;34m(instances, retriever, embedding_model, knowledge_index)\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m     '''\n\u001b[0;32m---> 23\u001b[0;31m     responses, _ = answer_with_rag(\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mquestions\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-8111f3de94b9>\u001b[0m in \u001b[0;36manswer_with_rag\u001b[0;34m(questions, llm, prompt_template, sampling_params, knowledge_index, embedding_model, reranker, num_retrieved_docs, num_docs_final, printing, retriever)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprinting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=> Generating answers...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_docs_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-5d3695be1847>\u001b[0m in \u001b[0;36mread\u001b[0;34m(llm, sampling_params, prompt_template, contexts, questions)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# Generate answers using the LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Extract the generated text from the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-954402e62709>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, sampling_params)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             response = self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/together/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, max_tokens, stop, temperature, top_p, top_k, repetition_penalty, presence_penalty, frequency_penalty, min_p, logit_bias, seed, stream, logprobs, echo, n, safety_model, response_format, tools, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         ).model_dump(exclude_none=True)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         response, _, _ = requestor.request(\n\u001b[0m\u001b[1;32m    142\u001b[0m             options=TogetherRequest(\n\u001b[1;32m    143\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/together/abstract/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, options, stream, remaining_retries, request_timeout)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     ]:\n\u001b[0;32m--> 242\u001b[0;31m         result = self.request_raw(\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mremaining_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremaining_retries\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/together/abstract/api_requestor.py\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, options, remaining_retries, stream, request_timeout, absolute)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             result = _thread_context.session.request(\n\u001b[0m\u001b[1;32m    492\u001b[0m                 \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0mabs_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"def combine(partialFilePath):\n  num = 100\n  predictions = {}\n  triviaqa_instances = {\n    \"Data\": [],\n    \"Domain\": \"Wikipedia\",\n    \"VerifiedEval\": False,\n    \"Version\": 1.0,\n  }\n\n  for i in range(1,6):\n    # Define the file path\n    inst_path = partialFilePath + \"_triviaqa_instances_\" + str(i*num) + \".json\"\n    pred_path = partialFilePath + \"_triviaqa_predictions_\" + str(i*num) + \".json\"\n    # Load the instances from the JSON file\n    with open(inst_path, 'r') as f:\n      loaded_instances = json.load(f)\n      triviaqa_instances['Data'] = triviaqa_instances['Data'] + loaded_instances['Data']\n    # Load the predictions from the JSON file\n    with open(pred_path, 'r') as f:\n      loaded_predictions = json.load(f)\n      predictions.update(loaded_predictions)\n  return predictions, triviaqa_instances","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:21:57.145330Z","iopub.execute_input":"2025-04-15T20:21:57.145720Z","iopub.status.idle":"2025-04-15T20:21:57.151644Z","shell.execute_reply.started":"2025-04-15T20:21:57.145688Z","shell.execute_reply":"2025-04-15T20:21:57.150572Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"predictions, triviaqa_instances = combine(\"/kaggle/working/result_500/zephyr_faiss\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:21:58.827115Z","iopub.execute_input":"2025-04-15T20:21:58.827484Z","iopub.status.idle":"2025-04-15T20:21:58.840891Z","shell.execute_reply.started":"2025-04-15T20:21:58.827433Z","shell.execute_reply":"2025-04-15T20:21:58.839922Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Save the list of instances as s JSON file\nwith open('/kaggle/working/result_500/zephyr_faiss_triviaqa_instances.json', 'w') as f:\n    json.dump(triviaqa_instances, f, indent=4)\n# Save the list of predictions as s JSON file \nwith open('/kaggle/working/result_500/zephyr_faiss_triviaqa_predictions.json', 'w') as f:\n    json.dump(predictions, f, indent=4) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:22:04.451634Z","iopub.execute_input":"2025-04-15T20:22:04.451957Z","iopub.status.idle":"2025-04-15T20:22:04.486869Z","shell.execute_reply.started":"2025-04-15T20:22:04.451934Z","shell.execute_reply":"2025-04-15T20:22:04.486187Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"!cd /kaggle/working/triviaqa && python3 -m evaluation.triviaqa_evaluation --dataset_file /kaggle/working/result_500/zephyr_faiss_triviaqa_instances.json --prediction_file /kaggle/working/result_500/zephyr_faiss_triviaqa_predictions.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:22:12.231761Z","iopub.execute_input":"2025-04-15T20:22:12.232077Z","iopub.status.idle":"2025-04-15T20:22:12.707386Z","shell.execute_reply.started":"2025-04-15T20:22:12.232053Z","shell.execute_reply":"2025-04-15T20:22:12.706486Z"}},"outputs":[{"name":"stdout","text":"em=0: Sunset ['sunset boulevard', 'sunset bulevard', 'west sunset boulevard', 'sunset blvd']\nem=0: Bonar ['henry campbell bannerman', 'sir henry campbell bannerman', 'campbell bannerman']\nem=0: None ['lauren becall', 'loren bacall', 'lauren becal', 'lauren bacall', 'betty j perske', 'betty perske', 'betty joan perske', 'bacall', 'betty joan perski']\nem=0: Fiddler ['fiddler on roof', 'sprintze', 'anatevka', '2 life', 'fiddler on reoof']\nem=0: Mutiny on the \" ['mutiny on bounty history', 'hms bounty mutineers', 'bounty vessel', 'mutiny on bounty', 'thomas ledward']\nem=0: Capote ['personhood theory', 'persons', 'person', 'perſon', 'perſons', 'person philosophical']\nem=0: Actor ['master builder occupation', 'graziani corazza', 'architecht', 'clifford lawrie', 'architechts', 'architects', 'registered architect', 'hok canada inc', 'architect', 'stanford downey architects inc']\nem=0: Stars on 45 Med ['stars on 45 song', 'stars on 45 single', 'stars on 45 medley', 'medley intro venus sugar sugar no reply i ll be back drive my car do you want to know secret we can work it out i should have known better nowhere man you re going to lose that girl stars on 45']\nem=0: No mention. ['south vancouver british columbia', 'vansterdam', 'vancouvr', 'vancouver bc', 'vancover', 'vancouver british colombia', 'hong kouver', 'vancouver', 'vancouver bc canada', 'vancouverites', 'un locode cavan', 'vancouver christian school', 'vancouver b c canada', 'vancouver city centre british columbia', 'vanocuver', 'vancouver british columbia', 'corpus christi elementary', 'vancouverite', 'hongcouver', 'hong couver', 'hastings east vancouver', 'vancouver canada', 'vancover british columbia', 'corpus christi elementary school canada', 'city of vancouver', 'greater vancouver bridges', 'vancouver b c', 'vancouver british columbia canada']\nem=0: Jelly ['ferdinand morton', 'jellyroll morton', 'joseph ferdinand morton', 'f p lamothe', 'ferdinand lamothe', 'ferdinand joseph la menthe', 'ferdinand joseph morton', 'ferdinand joseph lamothe', 'jelly roll morton', 'louise monette', 'ferdinand 22jelly roll 22 morton']\nem=0: Richard ['richard noble']\nem=0: Bill Cosby ['cleotha staples', 'staple singers', 'staples singers']\nem=0: Billie Jean King ['virginia wade', 'sarah virginia wade']\nem=0: Old ['hagen walter', 'walter hagen', 'walter charles hagen']\nem=0: Amundsen ['sir edmund', 'edmond hillary', 'sir edmund hillary', 'edmund hillary', 'sir edmund percival hillary', 'edmund hilary', 'sir ed', 'ed hillary', 'sir edmund hilary', 'edmund percival hillary']\nem=0: Seymour ['dark side of camelot', 'seymour sy myron hersh', 'chain of command book', 'seymour hersch', 'seymour m hersh', 'seymour hersh', 'sy hersh', 'hersh seymour m', 'chain of command road from 9 11 to abu ghraib', 'seymour hirsch']\nem=0: Sebastian ['lydian stone', 'jaspis', 'egyptian jasper', 'youngite', 'bruneau jasper', 'jasper', 'jasper mineral', 'black jasper']\nem=0: Carl ['carl wilson discography', 'carl dean wilson', 'carl wilson', 'wilson carl dean', 'wilson carl']\nem=0: Mark ['colin lionel emm', 'richard dawson', 'colin emm', 'dickie dawson', 'dick dawson']\nem=0: A black bear ['ursidae', 'bear cub', 'cub bear', 'bear hibernation', 'ursoidea', 'bear', 'reproductive behavior of bears', 'honey pig', 'ursoid', 'bears', 'mating bears', 'ursine', '🐻', 'arctos', 'bear zoology', 'sexual behavior of bears']\nem=0: July 21 ['20 july', 'jul 20', 'historical anniversaries july 20', 'july 20th', 'july 20', '20th july']\nem=0: Photographer/Filmm ['photographr', 'photographist', 'freelance photographer', 'photographer', 'freelance photography']\nem=0: Christian ['keifer sutherland', 'kiefer sutherland', 'kiefer william frederick dempsey george rufus sutherland', 'kiefer sutherlund', 'kiefer sutherland characters', 'promised land 1987', 'keefer sutherland', 'kiefer william fredrick dempsey george rufus sutherland', 'keifer southerland']\nem=0: 9 ['10', 'ten']\nem=0: Miss Georgia Peach ['tyrus raymond cobb', 'cobb ty', 'ty cobb', 'tyrus cobb', 'georgia peach']\nem=0: Jimi ['johnny allen hendrix', 'jimmy hendrix', 'jim hendrix', 'jimi hendrix', 'jimy hendrix', 'hendrix', 'jimmy hendricks', 'early life of jimi hendrix', 'electric church', 'gypsy suns and rainbows', 'heaven research', 'villanova junction', 'james hendrix', 'al hendrix', 'james marshall hendrix', 'gypsy sun and rainbows', 'lithofayne pridgeon', 'janie hendrix', 'jimmi hendrix']\nem=0: Donny ['donald osmond', 'donald clark osmond', 'donny marie in las vegas', 'donnie osmond', 'donny osmond']\nem=0: Beekeeper ['bee keepers', 'bee keeping', 'beekeeping', 'beekeeping helmet', 'bee farming', 'beekeeping suit', 'apairist', 'apiculture', 'history of beekeeping', 'beekiping', 'bee keeper', 'bee culture', 'beekeeping leading practices', 'apiarian']\nem=0: Paul Rudd ['jennifer joanna anastassakis', 'jenifer aniston', 'aniston jennifer', 'jennifer pitt', 'counter clockwise film', 'jenifer anniston', 'pumas film', 'jennifer anistion', 'jennifer aniston', 'jen aniston', 'jennifer anniston', 'jennifer joanna aniston', 'jennifer anastassakis']\nem=0: Jonathan ['roger william corman', 'doorway film', 'roger corman']\nem=0: Bibi ['isotta ingrid rossellini', 'ingrid rossellini', 'ingrid berman', 'ingrid bergmann', 'ingrid bergman']\nem=0: Full ['full monty film', 'full monty soundtrack', 'full monty']\nem=0: Bette ['crawford', 'crawford village', 'crawford disambiguation']\nem=0: James ['james stewart actor', 'jimmy stuart', 'stewart james', 'jimmy stewart', 'james stewart']\nem=0: Faye ['fred astairey', 'fred astare', 'fred astair', 'frederick austerlitz astaire', 'fred austerlitz', 'phyllis potter', 'fred astaire', 'frederick austerlitz']\nem=0: Niece ['daughters', 'daughter', 'daughterhood']\nem=0: Martina ['hingis', 'swiss miss', 'martina hingis', 'martina hingisova', 'martinahingis', 'martina hingisová']\nem=0: 17 ['13', 'thirteen']\nem=0: Guess Whos Coming To ['guess whose coming to dinner', 'guess who s coming to dinner 3f', 'guess who s coming to dinner']\nem=0: Gary Lewis ['gary lewis and playboys', 'dave gonzalez gary lewis playboys', 'gary lewis playboys']\nem=0: collapsible baby buggy ['baby buggy']\nem=0: John ['harrison ford', 'ford harrison', 'harison ford', 'harrison ford actor']\nem=0: Dodgers ['phillies', 'philadelphia phyllis', 'phillies roster', 'philadelphia phillies sports', 'phightin phils', 'philadelphia phillie', 'philadelphia phil', 'philadelphia phillis', 'phillies nation', 'philadelphia phillies quakers', 'phills', 'list of philadelphia phillies captains', 'filies', 'philadelphia blue jays', 'philadelphia phillies roster', 'pillies', 'philadelphia phillies', 'phils']\nem=0: Defending Champion ['twelve', '12']\nem=0: The Roman Spring of Mrs ['splendor in grass']\nem=0: Debra ['deborah winger', 'debra winger', 'mary debra winger']\nem=0: Amos 'n' ['amos andy', 'brazilian brass mines', 'george 22kingfish 22 stevens', 'amos and andy', 'amos n andy show', 'amos n andy']\nem=0: Lennie Weinrib ['john astin']\nem=0: None ['florence hendersen', 'florence agnes henderson', 'florence henderson', 'flo henderson']\nem=0: Roger Taylor ['sampras', 'sampras pete', 'peter sampras', 'pete sampras', 'petros 22pete 22 sampras']\nem=0: Chinese national football team ['to suijutsu', 'swam', 'swimming disambiguation', 'swimming']\nem=0: French Open ['french disambiguation', 'french', 'frenchness', 'francaise', 'francais', 'frrench', 'français']\nem=0: 7 ['three', '3']\nem=0: Unsolicited ['junkmail', 'junkmailing', 'junk mail', 'junk mail disambiguation']\nem=0: Frank ['frank harris']\nem=0: None ['teaching of rapture', 'pre tribulation rapture', 'rapture protestant belief', 'midtribulation rapture', 'rapture', 'pretribulation rapture', 'pre tribulational', 'mid tribulation rapture', 'midtribulationism', 'pretribulationistism', 'pre trib', 'pre tribulation']\nem=0: Elvis ['pressly', 'presley', 'pressley']\nem=0: Yasser ['death of yasser arafat', 'محمد عبد الرحمن عبد الرؤوف عرفات القدوة الحسيني', 'yasir arafat disambiguation', 'abū 60ammār', 'yassir arafat', 'illness of yasser arafat', 'yassar arafat', 'abu ammar', 'muhammed al qudwa al husseini', 'yasir arafat', 'ashraf al kurdi', 'muhammad abd ar rauf al qudwah al husayni', 'yasser arafet', 'yasser arafat', 'yaser arafat', 'mohammed abdel raouf arafat as qudwa al hussaeini', 'mohammed abdel rahman abdel raouf arafat al qudua al husseini', 'mohammed abdel rahman abdel raouf arafat al qudwa al husseini', 'death of arafat', 'jassir arafat', 'abu amar']\nem=0: Fairbanks ['chaplin disambiguation', 'chaplin', 'chaplin musical']\nem=0: 2% ['8', 'eight']\nem=0: Julie ['julie elizabeth wells', 'julie elizabeth andrews dbe', 'julie elizabeth andrews', 'julia elizabeth wells', 'julie andrews', 'julie wells', 'dame julie andrews', 'julie andrews edwards']\nem=0: Lisa Marie Presley ['debbie jackson', 'deborah rowe jackson', 'debbie rowe', 'deborah jeanne rowe', 'deborah rowe']\nem=0: Wilmington ['steel city', 'climate of pittsburgh pennsylvania', 'pittsbrugh', 'un locode uspit', 'pittsburgh', 'pittsburgh frick 6–8 middle school', 'pittsburgh pennsylvania usa', 'pittsburgh style', 'city of pittsburgh', 'pittsburgh pennsylvania u s', 'st justin s high school', 'pittsburgh pa', 'glenwood pennsylvania', 'da burgh', 'pittsburgh style of literature', 'pitsburgh', 'east end pittsburgh', 'pittsburgh pennsylvania us', 'pittsburgh usa', 'smoky city', 'city of bridges', 'fort du quesne', 'pittsburg pennsylvania', 'pittsburgh frick 6 8 middle school', 'pittsburgh pennsylvania', 'pittsburgh united states of america', 'education in pittsburgh', 'pittsburg pa', 'pittsburgh pennsyvania', 'burgh', 'frick international studies academy middle school', 'pittsburgh allegheny county pennsylvania', 'pittsburgh pgh']\nem=0: Heinrich ['himler s evil genius', 'heidrich reinhart', 'heidrich rhinehard', 'reinhart heydrich', 'reinhard heydrich', 'himmler s evil genius', 'heidrich rinehart', 'heidrich reinhard', 'reinhard hydrich', 'reinhard heydrick', 'reinhardt heydrich', 'rinehard heidrich', 'reinhart heidrich', 'rhinehardt heydrich', 'rhinehart heidrich', 'heydrichreinhard', 'reinhardheydrich', 'rinehard heydrich', 'reinhard heidrich', 'reihnard heidrich', 'heydrich reinhardt', 'heydrich reinhard', 'heydrich reinhart', 'reinhard hidrick', 'rhinehardt heidrich', 'rhinehard heidrich', 'butcher of prague', 'rinehart heidrich', 'heidrich rhinehart', 'heydrich', 'reinhard hidrich', 'reinhard hydrick', 'rhinehart heydrich', 'heidrich rinehardt', 'rheinhard heydrich', 'rinehart heydrich', 'rhinehard heydrich', 'young evil god of death', 'reinhard tristan eugen heydrich', 'rinehardt heydrich', 'heydrichreinhardt', 'reinhardtheydrich', 'heidrich rinehard', 'rinehardt heidrich', 'heidrich reinhardt', 'heidrich rhinehardt']\nem=0: Florida ['texas', 'religion in texas', 'texos', 'christianity in texas', 'texass', 'texas u s state', 'us tx', 'lake ozark texas', 'twenty eighth state', 'state of texas', 'lone star state', 'texas state', 'texas sized', '28th state', 'texas usa', 'texas united states', 'everything is bigger in texas']\nem=0: Belle Elmore ['his wife']\nem=0: New York City ['us ny', 'n y', 'neo york', '11th state', 'new york state s', 'nýja jórvík', 'new yourk', 'new york us', 'new york', 'estado nueva york', 'nova york', 'ny state', 'state new york', 'new yourk state', 'u s state of new york', 'eleventh state', 'state of n y', 'new york u s', 'new york s', 'n y u s state', 'newyork', 'new york usa state', 'new yrok', 'state of new yourk', 'state of new york', 'nyja jorvik', 'ny', 'estado de nueva york', 'religion in new york', 'nueva york estado', 'new york u s state', 'new york state', 'state of ny', 'n y state', 'us state of new york', 'nys', 'new york population', 'ny u s state']\nem=0: Napoleon isn't ['buchan john', 'buchan john sir', 'sir john buchan', 'john scantlebury blenkiron', 'lord tweedsmuir', 'john buchan 1st baron tweedsmuir', 'john buchan', 'baron tweedsmuir of elsfield', 'john buchan baron tweedsmuir', 'john sir buchan', 'buchanesque', 'john s blenkiron', 'john blenkiron']\nem=0: Wakhan ['kyber pass', 'khyber pass', 'درہ خیبر', 'khyber gate', 'khaibar pass', 'khaiber pass', 'kaiber pass']\nem=0: Slavic ['bulgarian disambiguation', 'bulgarian']\nem=0: Third ['kings crusade', '3rd crusade', 'third crusade', 'crusade of 1189']\nem=0: Hastings ['battle at hastings', 'battle of 1066', 'battle of harstings', 'hastings battle of', 'hastings battle', 'battle of hastings', 'battle of senlac']\nem=0: None ['queen catherine parr', 'kateryn parre', 'katheryn parre', 'queen katherine parr', 'lady latimer', 'katharine parr', 'katharine parre', 'catherine borough', 'catherine parr', 'katherine borough', 'catherine parre', 'katherine parr', 'anthony martienssen', 'lady lattimer', 'kateryn parr', 'katheryn parr', 'katherine parre']\nem=0: Ecology Party \nPE ['green party of united kingdom', 'green party 1985 90', 'uk green party', 'british green party', 'green parties uk', 'green party of great britain', 'green party 1985–1990', 'people movement', 'green party uk', 'people party uk 1973 75', 'ecology party', 'green parties united kingdom', 'green party united kingdom']\nem=0: First-Past-The- ['fptp', 'simple majority', 'multiple member first past post voting', 'first pass post', 'first past post election system', 'first past post electoral system', 'first past post method', 'first past post system', 'first past post voting', 'first past post fptp', 'first past post']\nem=0: Jingoism ['chauvinist', 'chauvinistic', 'chauvanistic', 'chauvenism', 'male chauvinist', 'chauvanism', 'male chauvinism', 'chauvinist pig', 'chauvanist', 'male chauvinists', 'male chauvinist pigs', 'female chauvinism', 'male chauvinist pig', 'chauvinism']\nem=0: Môme Piaf ['little sparrow']\nem=0: Silica ['sand grain', 'floor sugar', 'desert sand', 'sandiest', 'sand', 'coversand', 'psammophilous', 'grains of sand', 'psammophile']\nem=0: Eukaryote ['fungii', 'mycota', 'multicellular fungi', 'sexuality of fungi', 'fungis', 'eumycetes', 'fungal proteins', 'fungi', 'mycete', 'funghi', 'fungal growth', 'mycetes', 'eumycota', 'fungi kingdom', 'fungal development', 'antigens fungal', 'fungal', 'antibodies fungal', 'fungus kingdom', 'kingdom fungi', 'fungus', 'fugus', 'mycetae', 'fungus plants']\nem=0: Vaporization ['evaporates', 'vaporized', 'evaporate', 'evaporation', 'vaporizing', 'evaporating', 'evaporated', 'evapouration', 'evapourate', 'vaporizes', 'vapourization', 'verdunstung', 'évaporation', 'vaporise', 'vapourise']\nem=0: Mathematics ['arithmetic operations', 'arithmetics', 'arithmetic operators', 'history of arithmetic', 'arithmetic', 'additive operator', 'arithmatic', '−×÷', 'x', 'arithmetic operation', 'multiplicative operator', 'rithmetic', 'arithmetician', 'arithmetic operator', 'arithmetik', 'aritmetika', 'arithmetical operations']\nem=0: Termitidae ['ant', 'ant trails', '🐜', 'drone ant', 'formicidae', 'ants communication', 'history of ants', 'giant ant', 'ergate', 'ants', 'ant trail', 'worker ant']\nem=0: Wisdom ['third maxillary molar', 'impaction dental', 'maxillary third molar', 'mandibular third molar', '3rd molar', 'impacted molar', 'wisdom teeth', 'third molar', 'wisdom tooth', 'third mandibular molar', 'molar third', 'third molar tooth']\nem=0: Helium ['dioxygen molecule', 'molecular oxygen', 'atcvet code qv03an01', 'element 8', 'e948', 'sauerstoff', 'oxyjunn', '0xygen', 'diatomic oxide', 'pure oxygen', 'o element', 'atomic number 8', 'oxigen', 'history of oxygen', 'oxygen', 'oxygyn', 'oxygen atom', 'atc code v03an01', 'vital air', 'oxygen partial pressure', 'oxygen rings', 'oxygen ion', 'o₂', 'active oxygen', 'diatomic oxygen', 'oxygen gas']\nem=0: Strontium ['household radon', 'thoron', 'radium emanation', 'radon element', 'radon rn', 'element 86', 'actinon', 'niton element', 'radon poisoning', 'radon', 'radon gas', 'emanation chemistry']\nem=0: Galaxy ['speed of milky way through space', 'local galaxy', 'milky way galaxy', 'milky way astronomy', 'milky way', '🌌', 'our galaxy', 'milky way band of light', 'this galaxy', 'age of milky way', 'milkyway', 'via lactea', 'lactea', 'spiral arms of milky way', 'galaxia kuklos']\nem=0: Manchester United ['keltic', 'celtic disambiguation', 'celtic', 'celto']\nem=0: Northern Lights ['denali south buttress', 'большая гора', 'mount mckinley south buttress', 'mount mckinley alaska', 'denali', 'mt denali', 'mount mckinely', 'mount mckinley', 'denali north peak', 'highest mountain in north america', 'mount mckinley east buttress', 'mt mckinley denali', 'mount mckinley north peak', 'churchill peaks', 'mount denali', 'mt mckinley', 'denali east buttress']\nem=0: citric ['3 hydroxypentanedioic acid 3 carboxylic acid', 'citrus acid', 'hydrogen citrate', 'sour salt', 'hoocch2 oh c cooh ch2cooh', 'citric acid', '2 hydroxy 1 2 3 propanetricarboxylic acid', 'e 330', 'e330', 'acid of lemon', 'atc code a09ab04', 'atcvet code qa09ab04']\nem=0: Central ['jubilee line', 'fleet line']\nem=0: Oscar ['o flahertie', 'theocritus villanelle', 'flahertie', 'oscar wilde biblio', 'oscar fingal o flahertie wills wilde', 'o wilde', 'oscar fingal o flaherty wills wilde', 'oscar wilde', 'oscar o flaherty wilde', 'c 3 3 3', 'sebastian melmoth', 'oscar o flahertie wills wilde', 'oscar wild', 'cyril wilde']\nem=0: No ['true song', 'true disambiguation', 'truer', 'true', 'true album']\nem=0: C ['c and f']\nem=0: Belgium ['der schweiz', 'switzeland', 'confederation helvetia', 'confederatio helvetica', 'swissland', 'switzerland information', 'confederation suisse', 'switserland', 'confédération suisse', 'swizterland', 'confederation of helvatia', 'swizerland', 'svizzera', 'confederation helvetica', 'land of switzers', 'suiza', 'confederaziun svizra', 'schweiz', 'etymology of switzerland', 'die schweiz', 'suisse', 'confederazione svizzera', 'swiss', 'svizra', 'switzerland', 'schweitz', 'swiss confederation', 'switz', 'svissland', 'schwitzerland', 'environmental integrity group', 'land of swiss', 'schweizerische eidgenossenschaft', 'swissenergy', 'confœderatio helvetica', 'confoederatio helvetica', 'switzer land', 'švýcarsko', 'swiss confederated states', 'iso 3166 1 ch']\nem=0: Prisoner of Azk ['hp 3', 'third harry potter book', 'harry potter and prisoner of azkaban book', 'prisoner of azkaban', 'harry potter and prisoner of azkaban', 'hp3', 'harry potter iii', '3rd harry potter book', 'harry potter prisoner of azkaban', 'hppoa', 'harry potter and prisoner of azkhaban', 'harry potter 3', 'hary potter and prisoner of azkaban']\nem=0: Rupturewort ['type wrote', 'type bar', 'typewriter', 'electronic word processor', 'typewriter eraser', 'manual typewriter', 'personal word processor', 'type machine', 'typewriting', 'typewrites', 'typebars', 'type writer', 'type writers', 'typebar', 'type written', 'typewrote', 'type writing', 'electric typewriter', 'type write', 'typewriter ribbons', 'typing machine', 'first typewriter', 'typebasket', 'type writes', 'typewriterly', 'typewriter carriage', 'electronic typewriter', 'typewrite', 'typewriters', 'typewriter keyboard', 'electromechanical typewriter', 'type writerly', 'typewritten']\nem=0: Huey, Dew ['huey duey and louis', 'huey dewey and louie', 'ripp rapp og thorfinnur', 'huey dewey', 'huebert duck', 'donald duck s nephews', 'louie duck', 'huey dewey and louie filmography', 'huey duck', 'louis duck', 'huey dewey and louie duck', 'huey louie and dewey', 'dewey duck', 'ripp rapp og þorfinnur', 'phooey duck', 'deuteronomy duck', 'huey dewie and louie', 'huey duey and louie', 'ripp rapp og rupp', 'huey dewey louie']\nem=0: Manhattan ['sleepless in seattle original motion picture soundtrack', 'sleepless in seattle', 'sleepless in seattle film']\nem=0: Jack ['jack rippers victims', 'ripperologist', 'ripperology', 'jack ripper non fiction', 'ada wilson', 'leather apron', 'annie millwood', 'pinchin street murder', 'ripperologists', 'fairy fay', 'jack ripper', 'leatherapron', 'annie farmer', 'whitechapel murderer']\nem=0: Century ['nelson disambiguation', 'nélson', 'nelson film', 'nelson']\nem=0: Racercar ['race driver', 'autorace', 'automobile racing', 'automobile race', 'race car driver', 'autosports', 'racecar driver', 'autoracing', 'auto racing', 'racecars', 'race cars', 'racing car', 'automobile racer', 'auto race', 'car racing', 'racing driver', '🏎', 'car race', 'race car', 'racing cars', 'racecar', 'automotive racing']\nem=0: Two ['three', '3']\nem=0: Roy ['bertha franklin', 'i thank god', 'dale cook singer', 'sam cooke']\nem=0: Fred ['fred j perry', 'fred perry', 'frederick john perry']\nem=0: Nadia ['nadia elena comăneci', 'comăneci', 'nadia comenici', 'nadia elena comaneci', 'comaneci nadia', 'comaneci', 'nadia comeneci', 'nadia comaneci', 'nadia comăneci']\nem=0: 11 years and 302 ['11 years and 302 days']\nem=0: Nicholas ['nick berry']\nem=0: Budweiser ['duff', 'duff disambiguation']\nem=0: James ['james i of england', 'james vi of scotland and england', 'king james vi', 'king james vi i', 'james vi of scotland', 'king james i', 'james i of great britain', 'james vi scotland', 'james vi of scotland james i of england', 'king james first', 'james i', 'king james vi of scotland and i of england', 'king james vi of scotland', 'james vi of england', 'james vi king of scots', 'james i king of england', 'james vi i', 'king james i of england', 'james vi of scotland and i of england', 'seumas vi of scotland', 'james i of england and james vi of scotland', 'james vi and i', 'james i and vi', 'james i of england and vi of scotland', 'james 6th', 'james i of uk', 'king james vi and i', 'james i of united kingdom', 'james i of ireland', 'wisest fool in christendom', 'james i england', 'james i of england scotland and irerland', 'james i of wales', 'james king of england ireland and scotland', 'wisest fool', 'king james i of england and vi of scotland', 'james vi', 'james i of england scotland and ireland', 'james first']\nem=0: CD:UK ['word', 'word film', 'word disambiguation']\nem=0: Maria ['marija sarapova', 'marija šarapova', 'мари́я ю́рьевна шара́пова', 'masha sharapova', 'мария шарапова', 'mariya sarapova', 'maria sugarpova', 'maria scharapowa', 'maria sharapova model', 'мари́я шара́пова', 'sugarpova', 'sugarpova com', 'maria sharpova', 'maria shirapova', 'maria sharapova', 'maria sharapova career history', 'maria sjarapova', 'maria szarapowa', 'maria yuryevna sharapova', 'sharapova']\nem=0: None ['three', '3']\nem=0: Scutts ['bowie disambiguation', 'bowie']\nem=0: 1500 meters ['1500 metres', 'one thousand five hundred distance', '1500 distance']\nem=0: James ['list of awards and nominations received by james cameron', 'james cameron', 'james francis cameron']\nem=0: Marilyn Monroe ['richard henry parkin starkey jr', 'richard starrkey', 'richie snare', 'richard starkey jr', 'ringo star', '22ringo 22', 'ringo starkey', 'starr ringo', 'richard starkey', 'ringo', 'ringo film', 'ringo starr', 'beatle ringo', 'richard starky']\nem=0: Meg ['margrit emily mary hyra', 'margaret mary emily anne hyra', 'margaret mary emily hyra', 'margaret emily hyra', 'kathryn sedgwick', 'margrit emily hyra', 'meg ryan', 'margaret hyra', 'margaret emily mary hyra', 'margrit hyra', 'margaret mary hyra', 'margrit mary hyra', 'margrit mary emily hyra', 'megan ryan']\nem=0: Nigel ['john roy major', 'major administration', 'bastardgate', 'majorism', 'major john', 'premiership of john major', 'john major', 'major john roy']\nem=0: Ritchie ['richard steven valenzuela', 'richie valenzuela', 'ritchie valens', 'ritchie valenzuela', 'richie valens', 'ritchie s valens', 'valens ritchie']\nem=0: Insect ['bats', 'bat wing', 'cheiroptera', 'artificial bat roost', 'chiropterology', 'flinder mouse', 'chiropter', 'chiropterologist', 'chiroptera', 'bat groups', 'bat species', 'bat evolution', 'bat life expectancy', 'chiroptologist', 'chiropteras', 'flittermouse', 'bat roost', 'bat', 'barotrauma and wind turbines', 'chiropteran', 'bat conservation', 'bat animal']\nem=0: Neruda ['neftalí ricardo reyes basoalto', 'ricardo eliecer neftalí reyes basualto', 'neftali ricardo elecier reyes basoalto', 'ricardo eliécer neftalí reyes basoalto', 'neftalí reyes', 'neruda pablo', 'ricardo eliecer neftalí reyes basoalto', 'neftali ricardo reyes basoalto ricardo eliecer neftali reyes y basoalto', 'ricardo eliecer naftalí reyes', 'ricardo eliecer neftali reyes basoalto', 'ricardo eliecer neftali reyes basualto', 'neftali ricardo reyes basoalto', 'pablo neruda', 'ricardo eliezer neftalí reyes basoalto', 'pablo neftali ricardo reyes basoalto neruda', 'ricardo eliecer naftali reyes', 'neftali reyes', 'ricardo eliecer neftali reyes y basoalto']\nem=0: Jimmy ['jimmy carr', 'joke technician', 'karoline copping', 'jimmy car']\nem=0: Gladiators ['fort boyard disambiguation', 'fort boyard']\nem=0: Tottenham ['armoury military', 'armoury', 'armory military', 'arsenals', 'arsenal ammunition', 'arsenal weaponry', 'arsenal']\nem=0: Kenneth ['kenneth brannagh', 'kenneth charles branagh', 'branagh', 'branagh kenneth', 'kenneth branagh', 'sir kenneth branagh', 'ken branagh', 'kenneth brannaugh']\nem=0: Geena ['geena davis', 'virginia elizabeth 22geena 22 davis', 'reza jarrahy', 'virginia elizabeth davis', 'gina davis', 'gena davis']\nem=0: Adrian Cronauer ['adrian', 'adrián']\nem=0: Billy ['bill wilder', 'billy wilder s', 'billy wilder']\nem=0: Big Brother 4 ['james andrew innes dee', 'james andrew innes 22jack 22 dee', 'jack dee s saturday night', 'dee jack', 'jack dee', 'jack dee s sunday service']\nem=0: Bernardinho ['eddie merkx', 'edouard louis joseph baron merckx', 'eddie merx', 'eddy merckx', 'eddie merckx', 'eddy merx', 'eddy merkx', 'edward merckx', 'eddy merckx cyclist']\nem=0: Captain America ['captain americas', 'runaways comics story arcs', 'captain america s', 'runaways comics', 'fantastic fourteen', 'runaways film', 'runaways comic']\nem=0: Never. ['none', 'none disambiguation']\nem=0: The Hand ['dressed to kill disambiguation', 'dressed to kill song', 'dressed to kill', 'dressed to kill movie', 'dressed to kill film']\nem=0: Teddy Sheringham ['denis law']\nem=0: Stuart ['macmalcolm', 'canmore dynasty', 'canmores', 'dunkeld dynasty', 'house of canmore', 'house of dunkeld']\nem=0: Norbert ['basilisk harry potter', 'firenze centaur', 'dobby house elf', 'ronan harry potter', 'sir patrick delaney podmore', 'headless hunt', 'society for promotion of elvish welfare', 'hedwig owl', 'pigwidgeon', 'pygmy puff', 'nagini harry potter', 'dobby house elf harry potter', 'gurg', 'thestrals', 'veela harry potter', 'winky house elf', 'peeves', 'blast ended skrewt', 'society for promotion of elfish welfare', 'nargle', 'ragnok', 'magizoology', 'kreachers', 'niffler', 'trevor harry potter', 'tom riddle s basilisk', 'dementor s kiss', 'hedwig harry potter', 'goblin harry potter', 'soaofmccctls', 'heliopath', 'firenze harry potter', 'stop outrageous abuse of our fellow magical creatures and campaign for change in their legal status', 'magical creatures harry potter', 'magorian', 'crumple horned snorkack', 'troll harry potter', 'hermes harry potter', 'thestral', 'dementor', 'hungarian horntail', 'errol harry potter', 'winged horse harry potter', 'pets in harry potter series', 'goblins harry potter', 'magical beasts harry potter', 'kreacher', 'dementors', 'dementoid', 'house elves', 'ghosts in harry potter', 's p e w', 'crookshanks', 'werewolf harry potter', 'non human characters in harry potter', 'bowtruckle', 'magical creatures in harry potter', 'house elf', 'boggart harry potter', 'dobby harry potter', 'griphook', 'kneazle', 'minor harry potter beasts', 'bane harry potter', 'pets from harry potter', 'minor beasts in harry potter', 'ghosts harry potter', 'cornish pixies']\nem=0: Richard ['richard st john harris', 'richard harris', 'richard st j harris', 'richard harris actor', 'richard saint john harris']\nem=0: PeterSutcliffe ['harold shipman', 'dr harold frederick shipman', 'dr harold shipman', 'dr death harold shipman', 'doctor shipman', 'harold frederick shipman', 'primrose shipman', 'fred shipman', 'dr shipman']\nem=0: Prince Edward, Earl of ['earl of wessex', 'hrh earl of wessex', 'edward anthony richard windsor', 'prince edward earl of wessex', 'earl and countess of wessex', 'edward windsor earl of wessex', 'prince edward', 'hrh prince edward', 'edward wessex', 'prince edward 1st earl of wessex', 'edward 1st earl of wessex', 'edward earl of wessex', 'prince edward of united kingdom', 'ardent productions']\nem=0: Christie ['john christie', 'john christie footballer', 'john christie disambiguation', 'christie john']\nem=0: Sirhan ['sir han sir han', 'sirhan bishara sirhan', 'sirhan sirhan murderer', 'sirhan b sirhan', 'sirhan sirhan']\nem=0: Hippety ['hippity hopper', 'giant mouse', 'hippety hopper']\nem=0: False ['winds of tara', 'gone with wind novel', 'gone with teh wind', 'characters in gone with wind', 'charles hamilton gone with wind', 'gone with win', 'gone w wind', 'gwtw', 'gone with wind', 'mammy gone with wind']\nem=0: Anna Wintour ['vogue', 'vogue single', 'vogue song', 'vogue disambiguation']\nem=0: Pamela ['pamela lee', 'pamula anderson', 'pam andreson', 'pamala anderson', 'pamela lee anderson', 'pamela anderson', 'pamela andersson', 'miss february 1990', 'pamela anderson lee', 'pamela anderson house', 'pam anderson', 'pamela denise anderson']\nem=0: Conservative ['margaret hilda thatcher', 'margaret hilda thatcher baroness thatcher', 'baroness thatcher', 'margaret thatcher', 'margaret thatcher s', 'margaret tatcher', 'prime minister margaret thatcher', 'margaret thatcher day', 'maggie thatcher', 'mrs thatcher', 'm thatcher', 'thatcherian', 'lady thatcher', 'mrs finchley', 'baroness margaret thatcher', 'margaret thatcher library', 'margareth thatcher', 'margret thatcher', 'mrs t', 'baroness thatcher of kesteven', 'milk snatcher', 'margaret hilda roberts', 'margaret thatcer', 'margaret thacher', 'margeret thatcher', 'margaret thatcher baroness thatcher', 'margaret hilda roberts thatcher', 'mrs denis thatcher', 'margaret thatcher bibliography', 'lady t']\nem=0: Devolution \nEconomic ['tony blair sports foundation', 'tony blaire', 'anthony tony blair', 'toney blair', 'prime minister tony blair', 'family of tony blair', 'blairian', 'herr blair', 'rt hon tony blair', 'tony blaer', 'anthony charles lynton 22tony 22 blair', 'tony bliar', 'anthony charles lynton blair', 'anthony c l blair', 'tony blare', 'tony blair', 'education education education', 'education education and education', 'tonie blair', 'tony blairs', 'tony blair pm', 'tony blari', 'tonyblair', 'tony blair s private life', 'blair doctrine', 'rt hon anthony blair mp', 'rt hon anthony charles lynton blair', 'tony blear', 'president blair', 'tony balir', 'tory blair', 'anthony charles blair', 'antony blair']\nem=0: Cliff ['cliff thorburn', 'champagne cliff', 'rhett butler of green baize']\nem=0: Malcolm Marshall ['michael holding', 'mike holding']\nem=0: Ricky ['ethel skinner']\nem=0: None ['vinarborg', 'wien austria', 'vindobonensis', 'vienna porcelain', 'vienna state', 'habsburg austria', 'vindabona', 'city of vienna', 'gemeinderat of vienna', 'bécs', 'wienna', 'architecture of vienna', 'wien', 'demographics of vienna', 'vienna austria', 'vínarborg', 'vienna at 9', 'capital of austria', 'wene', 'government of vienna', 'un locode atvie', 'europe vienna', 'viena', 'vienna']\nem=0: Till ['till death us do part', 'else garnett', 'till death us do part british tv series', 'till death us do part uk tv series', 'til death us do part']\nem=0: Many ['whinnie pooh', 'vinnie pux', 'winnie pooh', 'winnie poo', 'pooh bear', 'winnie pooh tv series', 'pooh', 'poohbear', 'winnie pooh character', 'list of winnie pooh television series', 'poo bear', 'hunny', 'vinni pux', 'bear of very little brain', 'winnie sanders', 'winney pooh', 'vinnie pukh', 'winny pooh']\nem=0: Cadbury ['frosted flakes of corn', 'they re gonna taste great', 'cocoa frosted flakes', 'tony s cinnamon krunchers', 'kellog s frosted flakes', 'kellogg s frosties', 'tiger power', 'sugar frosted flakes', 'frosted rice', 'cinnamon krunchers', 'whole grain tiger power', 'zucaritas', 'frosted flakes', 'kellogg s frosted flakes', 'list of frosted flakes products', 'frosties']\nem=0: Two ['four', '4']\nem=0: Kevin Rowland ['jockie wilson', 'jocky wilson', 'jockey wilson']\nem=0: 21 ['20', 'twenty']\nem=0: Michelle ['michele ryan', 'michelle ryan', 'michelle ryans']\nem=0: James ['clarence leiter', 'felix lieter', 'cedar leiter', 'felix leiter']\nem=0: David ['nixon david', 'david nixon disambiguation', 'david nixon']\nem=0: Spencer ['spencer perceval kc', 'spencer perceval', 'spencer percival']\nem=0: Berliner ['compact diplomacy', 'compact', 'compact disambiguation']\nem=0: Lauryn ['dana elaine owens', 'lancelot h owens', 'latifah', 'rita owens', 'queen latifa', 'dana owens', 'list of queen latifah s awards and nominations', 'queen latifah']\nem=0: Basso ['ivan basso']\nem=0: Kylie ['lily loves', 'still alright tour', 'is it scary album', 'lily allen', 'lilyallenmusic com', 'lily alen', 'lily allan', 'lilly allan', 'lily rose beatrice cooper', 'lily cooper', 'lilly rose cooper', 'sheezus tour', 'lilly allen', 'maggie may dog', 'lily alan', 'lily allen band', 'lily rose beatrice allen', 'lily rose cooper']\nem=0: Billie ['holiday billie', 'lady day singer', 'billie halliday', 'billie holliday', 'eleanora fagan gough', 'billie holiday', 'frank deviese', 'billy holliday', 'billy holiday', 'eleanora fagan']\nem=0: Anne ['anne hathaway actor', 'anne hathaway filmography', 'adam shulman', 'hathaway effect', 'anne hathaway', 'anne hathaway actress', 'anne jacqueline hathaway', 'annie hathaway']\nem=0: Mediterranean ['jadransko more', 'pollution of adriatic sea', 'adriatic coast', 'mare hadriaticum', 'adriatic sea', 'jadransko morje', 'adriatic']\nem=0: Greta ['judy garland james mason', 'joey luft', 'virgina gumm', 'judy garland', 'frances ethel gumm', 'frances gumm']\nem=0: Jodie ['alicia foster', 'alicia christian foster', 'jodie foster', 'jodi foster', 'joe d foster', 'foster jodie', 'alicia christian 22jodie 22 foster', 'jody foster']\nem=0: Orion ['leo', 'pba leo awards']\nem=0: Fleet ['fleet river', 'fleet ditch', 'river fleet', 'fleet river london']\nem=0: Overlord ['allied invasion of france', 'battle for normandy', 'normandy breakout', 'invasion of france allies', 'normandy breakout campaign', 'operation overlord', 'operation overlord essay']\nem=0: Wayne ['wayne sleep']\nem=0: Battle of Britain ['italian job soundtrack', 'italian job', 'charlie croker', 'italian job 1969 film', 'italian job film']\nem=0: Hyde Park ['altamont', 'altamont disambiguation']\nem=0: 8 minutes and 19 ['about 8 minutes']\nem=0: Supermarine/Vickers-A ['supermarine aviation works ltd', 'supermarine', 'supermarine aviation']\nem=0: Peter Doohan ['patrick cash', 'cat pash', 'pat cash']\nem=0: Marriage Lines ['ever decreasing circles']\nem=0: Jane ['jane horrocks']\nem=0: Hazell ['james hazell']\nem=0: Donald ['allan anthony donald', 'allan donald']\nem=0: Reach the summit of Mount ['climb everest']\nem=0: Abraham ['lincoln s cabinet', 'president lincoln', 'president abe lincoln', 'abe lincoln', 'abraham lincoln and american civil war', 'presedent lincon', 'honest abe', 'abe licnoln', 'lincoln', 'lincoln abraham', 'abraham lincoln president', 'abaham lincoln', 'president abraham', 'abe lincolin', 'lincolnian', '16th president of united states', 'abaraham lincoln', 'abraham lincoln', 'lincoln president', 'abraham lincon', 'president abraham lincoln', 'rail splitter', 'abraham president', 'abe lincon', 'abrahm lincoln', 'aberham lincoln', 'great emancipator', 'abraham lincoln s life', 'honest abe lincoln']\nem=0: Columbus ['colon cristobal explorer', 'cristóvão colombo', 'cristoforo columbo', 'christofo columbo', 'admiral of ocean sea', 'cristopher columbus', 'topher columbus', 'columbus quincentennial', 'crostoforo colombo', 'columbus s', 'christophorus columbus', 'christovao columbo', 'cristoforo colubmo', 'christopher coloumbus', 'christopher columbus', 'colón cristóbal explorer', 'christoffa corombo', 'cristobol colon', 'cristopher colombus', 'christoper columbus', 'christopher columbus 1st voyage', 'christopher columb', 'chistopher colombus', 'cristovao colombo', 'cristoforo colombo', 'cristóbal colón', 'christopher colub', 'perceptions of columbus', 'christopher colombus', 'christobal colon']\nem=0: Snagglepuss ['top cat', 'boss cat', 'brain top cat character']\nem=0: Charles ['prince of wales charles', 'next king of australia', 'charles iii of united kingdom', 'charles philip arthur george mountbatten windsor', 'prince charles of united kingdom', 'next king of uk', 'prince charles prince of wales', 'charles duke of cornwal', 'charles of united kingdom', 'his highness prince charles', 'hrh prince of wales prince charles', 'charles windsor', 'monstrous carbuncle', 'charles philip arthur george', 'charles mountbatten windsor', 'next king of canada', 'hrh prince charles', 'hrh duke of rothesay', 'charles price of wales', 'princes charles', 'charles duke of rothesay', 'charles of edinburgh', 'hrh prince of wales', 'prince charles of wales', 'prince charles duke of cornwal', 'prince of wales', 'duke of cornwall', 'next king of united kingdom', 'prince charles of edinburgh', 'charles v of england', 'prince charles', 'prince charles wales', 'hrh duke of cornwall', 'charles duke of cornwall', 'charles philip arthur windsor', 'charles windsor prince of wales', 'hrh prince charles prince of wales', 'war of waleses', 'charles crown prince of britain', 'charles of wales', 'prince charles duke of rothesay', 'duke of rothesay', 'charles philip arthur george windsor mountbatten', 'hm duke of cornwall', 'prince charles duke of cornwall', 'hrh prince charles duke of rothesay', 'charles prince of wales']\nem=0: Indian Rupee ['rupia', 'rupee symbol', 'generic rupee sign', 'rupiya', '௹', 'rs', 'rupees', '૱', '₨', 'rupi', 'roupee', 'rupee sign', 'rupee', 'रु', 'रू']\nem=0: Snooty ['dennis mennis', 'dennis menace tv series', 'dennis menace', 'dennis menace disambiguation', 'denis menice', 'dennis menace television', 'denis menace']\nem=0: Bond ['ernie', 'premium bond', 'premium bonds', 'electronic random number indicator equipment']\nem=0: Three-wheeled automobile ['automobilism', 'automobil', 'cars', 'passenger vehicles', 'autos', 'environmental impact of automobiles', 'automobile', 'motorization', '🚗', 'motor car', 'automobiles', '🚘', 'car', 'automobles', 'motorcar', 'car automobile', 'm1 vehicle', 'self rolling carriage', 'cardoor', 'ottomobile', 'passenger vehicle', 'motorisation', 'self propelling carriage', 'automotive vehicle', 'environmental impact of cars']\nem=0: Boeing ['marine two', 'marine corps one', 'marine i', 'marine one', 'marine 1', 'marine corps 1']\nem=0: Anne ['anne bullen', 'anne boylen', 'anne boleyn 1st marchioness of pembroke', 'anna bollina', 'anne bolyn', 'ann bullen', 'anne boullant', 'queen anne boleyn', 'ann boleyn', 'anne boleyn', 'anne boelyn']\nem=0: Florence, South Carolina ['bush junior', 'bushian', 'george w budh', 'g dub', 'baby bush', 'bush ii', 'george w bush', 'gorge w bush', 'g walker bush', 'jorge w bush', 'george w bush jr', 'us president george w bush', 'george w bush street', 'goerge w bush', 'george bush junior', 'georgewbush', 'gw bush', 'george walker bush', '43rd pesident of united states', 'george w bush 43rd president of united states', 'george w buah', 'george bush 2000', 'international perception of george w bush', 'dubya bush', 'george bush jr', 'george w bush painter', 'george w', 'junior bush', 'bush george w', 'dubyuh', 'g w bush', 'w bush', 'george dubya bush', 'george bush ii', 'george bush 2', 'dubya', 'bushists', 'president bush 2000', '2004 republican presidential nominee', 'religious faith of george w bush', 'president bush 43', 'former president george w bush', 'bush 43', '43rd president of united states', 'g w b', 'george w bussh', 'bush jr', 'george wаlker bush', 'president george walker bush', 'bush younger', 'george bush 43rd u s president', 'george bush 43', 'george younger', '2000 republican presidential nominee', 'geroge w bush', 'president george w bush']\nem=0: Dogger Bank ['fishers', 'fisher disambiguation', 'fisher']\nem=0: Bismarck ['otto of bismarck', 'otto vanbismark', 'otto eduard leopold von bismarck count of bismarck schönhausen duke of lauenburg prince of bismarck', 'otto van bismark', 'von bismark', 'count von bismarck schoenausen', 'bismarckian', 'von bismarck', 'accident insurance bill of 1884', 'kaizer bismark', 'otto fürst von bismarck', 'otto furst von bismarck', 'otto eduard leopold fuerst von bismarck', 'otto eduard leopold von prince bismarck', 'otto eduard leopold bismarck', 'otto eduard leopold furst von bismarck', 'otto eduard leopold fürst von bismarck', 'kaiser bismark', 'health insurance bill of 1883', 'iron chancellor', 'otto bismarck', 'otto van bismarck', 'economy of germany under bismarck', 'prince otto edward leopold von bismarck', 'otto eduard leopold von bismarck prince bismarck', 'otto eduard leopold von bismarck', 'prince bismarck', 'count otto von bismarck', 'iron chancellor disambiguation', 'old age and disability insurance bill of 1889', 'otto von bismark', 'prince otto eduard leopold von bismarck', 'otto von blotto', 'otto von bismarck', 'otto fuerst von bismarck']\nem=0: Macbeth ['tragical history of hamlet', 'revenge of hamlett prince of denmarke', 'hamlet shakespeare', 'hamletian', 'man delights not me', 'marcellus hamlet', 'bernardo character', 'hamlet', 'tragical history of hamlet prince of denmark', 'tragicall historie of hamlet prince of denmarke', 'murder of gonzago', 'hamlet play', 'goodnight sweet prince', 'bernardo hamlet', 'hamlet of denmark', 'william shakespeare s hamlet', 'hamlet prince of denmark', 'tragedy of hamlet', 'tragedy of hamlet prince of denmark']\nem=0: George ['plays unpleasant', 'george barnard shaw', 'gb shaw', 'g bernard shaw', 'shaw george bernard', 'george bernard shaw', 'plays pleasant', 'g b shaw']\nem=0: Jefferson ['t jefferson', 'jefferson thomas', 'president jefferson', 'tomas jefferson', 'thomas jefforson', 'sage of monticello', 'thomas jeffersson', 'third president of united states', 'tom jefferson', 'jefferson administration', 'president thomas jefferson', 'thomasjefferson', 'thomas jefferson first inaugural address', 'thomas jefferson', '3rd president of united states', 'thomas jefferson second inaugural address']\nem=0: Thirty-five ['35 years of age']\nem=0: Martin ['martin peters']\n{'exact_match': 59.6, 'f1': 71.45984126984135, 'common': 500, 'denominator': 500, 'pred_len': 500, 'gold_len': 500}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def answer_without_rag(\n    questions: List[str],\n    llm,\n    prompt_template,\n    sampling_params: dict,\n    printing: bool = True,\n) -> List[str]:\n    \"\"\"\n    Answer questions without any retrieval (i.e., vanilla LLM). \n    \"\"\"\n    if isinstance(questions, str):\n        questions = [questions]\n\n    # Prepare empty context (or prompt with no knowledge injection)\n    contexts = [\"\"] * len(questions)\n\n    if printing:\n        print(\"=> Generating answers without retrieval...\")\n\n    answers = read(llm, sampling_params, prompt_template, contexts, questions)\n    return answers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_in_chat_format_no_context = [\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"Answer the question with only one word or the simplest possible response (e.g., a single number or a single word).\nDo NOT generate sentences, explanations, or additional context.\nStop immediately after providing the answer. Do not generate any further words or tokens.\nIf the context does not provide any useful information, answer the question based on your own knowledge.\nI am going to provide you five examples:\n\nQuestion: What is the capital of Kenya?\nAnswer: Nairobi\n---\nQuestion: What was the name of the pig leader in George Orwell's Animal Farm?\nAnswer: Napoleon\n---\nQuestion: Which artist created the Katzenjammer Kids?\nAnswer: Rudolph Dirks\n---\nQuestion: Who was Geena Davis's husband when they made the loss-maker Cutthroat Island?\nAnswer: Renny Harlin\n---\nQuestion: Who was married to Spandau Ballet's Gary Kemp and later to Jude Law?\nAnswer: Sadie Frost\n\n\"\"\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"Now here is the question you need to answer.\n\nQuestion: {question}\"\"\"\n    },\n]\n\nNO_RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n    prompt_in_chat_format_no_context, tokenize=False, add_generation_prompt=True\n)\nprint(NO_RAG_PROMPT_TEMPLATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 重定义该函数，适配不使用RAG的情况\ndef perform_inference_p(instances, retriever, embedding_model=None, knowledge_index=None):\n    # Extract questions and their IDs\n    questions = [instance[\"Question\"] for instance in instances]\n    question_ids = [instance[\"QuestionId\"] for instance in instances]\n\n    # 使用 RAG 或 非 RAG\n    if retriever == \"none\":  # 小写 'none'，代表不使用RAG\n        responses = answer_without_rag(\n            questions=questions,\n            llm=model,\n            prompt_template=NO_RAG_PROMPT_TEMPLATE,\n            sampling_params=sampling_params,\n            printing=False,\n        )\n    else:\n        responses, _ = answer_with_rag(\n            questions=questions,\n            llm=model,\n            prompt_template=RAG_PROMPT_TEMPLATE,\n            sampling_params=sampling_params,\n            knowledge_index=knowledge_index,\n            embedding_model=embedding_model,\n            reranker=None,\n            printing=False,\n            retriever=retriever,\n        )\n\n    torch.cuda.empty_cache()\n\n    # Return result dict\n    results = [{\"QuestionId\": qid, \"Answer\": answer} for qid, answer in zip(question_ids, responses)]\n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 100\nfor i in range(0, len(test_data), batch_size):\n\n    data = test_data.iloc[i:i+batch_size]\n    predictions, triviaqa_instances = parallel_inference(data, retriever=\"none\", embedding_model=embedding_model,\n        knowledge_index=KNOWLEDGE_VECTOR_DATABASE_TEST)\n    file_path_instances = f'/kaggle/working/result_no_rag/zephyr_faiss_triviaqa_instances_norag_{i+batch_size}.json'\n    file_path_predictions = f'/kaggle/working/result_no_rag/zephyr_faiss_triviaqa_predictions_norag_{i+batch_size}.json'\n  \n# Save the list of instances as s JSON file\n    with open(file_path_instances, 'w') as f:\n        json.dump(triviaqa_instances, f, indent=4)\n# Save the list of predictions as s JSON file\n    with open(file_path_predictions , 'w') as f:\n        json.dump(predictions, f, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def combine2(partialFilePath):\n  num = 100\n  predictions = {}\n  triviaqa_instances = {\n    \"Data\": [],\n    \"Domain\": \"Wikipedia\",\n    \"VerifiedEval\": False,\n    \"Version\": 1.0,\n  }\n\n  for i in range(1,6):\n    # Define the file path\n    inst_path = partialFilePath + \"_triviaqa_instances_norag_\" + str(i*num) + \".json\"\n    pred_path = partialFilePath + \"_triviaqa_predictions_norag_\" + str(i*num) + \".json\"\n    # Load the instances from the JSON file\n    with open(inst_path, 'r') as f:\n      loaded_instances = json.load(f)\n      triviaqa_instances['Data'] = triviaqa_instances['Data'] + loaded_instances['Data']\n    # Load the predictions from the JSON file\n    with open(pred_path, 'r') as f:\n      loaded_predictions = json.load(f)\n      predictions.update(loaded_predictions)\n  return predictions, triviaqa_instances","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions, triviaqa_instances = combine2(\"/kaggle/working/result_no_rag/zephyr_faiss\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the list of instances as s JSON file\nwith open('/kaggle/working/result_no_rag/zephyr_faiss_triviaqa_instances.json', 'w') as f:\n    json.dump(triviaqa_instances, f, indent=4)\n# Save the list of predictions as s JSON file \nwith open('/kaggle/working/result_no_rag/zephyr_faiss_triviaqa_predictions.json', 'w') as f:\n    json.dump(predictions, f, indent=4) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd /kaggle/working/triviaqa && python3 -m evaluation.triviaqa_evaluation --dataset_file /kaggle/working/result_no_rag/zephyr_faiss_triviaqa_instances.json --prediction_file /kaggle/working/result_no_rag/zephyr_faiss_triviaqa_predictions.json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 40 minutes","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
