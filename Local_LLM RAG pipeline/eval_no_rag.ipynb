{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8b9c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 15:00:50 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402e06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_json(\"../qa/wikipedia-dev.json\")\n",
    "sample_size = 500\n",
    "data = test_data.iloc[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d88590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 15:01:36 [config.py:585] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 04-14 15:01:37 [awq_marlin.py:114] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 04-14 15:01:37 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 04-14 15:01:37 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 04-14 15:01:37 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='AMead10/Llama-3.2-3B-Instruct-AWQ', speculative_config=None, tokenizer='AMead10/Llama-3.2-3B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=AMead10/Llama-3.2-3B-Instruct-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 04-14 15:01:38 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f5e305c24d0>\n",
      "INFO 04-14 15:01:38 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "WARNING 04-14 15:01:38 [interface.py:303] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 04-14 15:01:38 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-14 15:01:38 [gpu_model_runner.py:1174] Starting to load model AMead10/Llama-3.2-3B-Instruct-AWQ...\n",
      "WARNING 04-14 15:01:39 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 04-14 15:01:39 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 04-14 15:01:39 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3b8a676bd7430e93edc91a22925985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 15:01:42 [loader.py:447] Loading weights took 2.90 seconds\n",
      "INFO 04-14 15:01:43 [gpu_model_runner.py:1186] Model loading took 2.1364 GB and 4.133809 seconds\n",
      "INFO 04-14 15:01:45 [kv_cache_utils.py:566] GPU KV cache size: 11,424 tokens\n",
      "INFO 04-14 15:01:45 [kv_cache_utils.py:569] Maximum concurrency for 4,096 tokens per request: 2.79x\n",
      "INFO 04-14 15:01:45 [core.py:151] init engine (profile, create kv cache, warmup model) took 2.03 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM\n",
    "READER_MODEL_NAME = \"AMead10/Llama-3.2-3B-Instruct-AWQ\"\n",
    "model = LLM( \n",
    "    model = READER_MODEL_NAME,\n",
    "    tensor_parallel_size=1, \n",
    "    gpu_memory_utilization=1.0, \n",
    "    trust_remote_code=True,\n",
    "    enforce_eager=True,\n",
    "    disable_log_stats=True,\n",
    "    max_model_len=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee2eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template without context\n",
    "tokenizer = model.get_tokenizer()\n",
    "prompt_in_chat_format_no_context = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Answer the question with only one word or the simplest possible response (e.g., a single number or a single word).\n",
    "Do NOT generate sentences, explanations, or additional context.\n",
    "Stop immediately after providing the answer. Do not generate any further words or tokens.\n",
    "If the context does not provide any useful information, answer the question based on your own knowledge.\n",
    "I am going to provide you five examples:\n",
    "\n",
    "Question: What is the capital of Kenya?\n",
    "Answer: Nairobi\n",
    "---\n",
    "Question: What was the name of the pig leader in George Orwell's Animal Farm?\n",
    "Answer: Napoleon\n",
    "---\n",
    "Question: Which artist created the Katzenjammer Kids?\n",
    "Answer: Rudolph Dirks\n",
    "---\n",
    "Question: Who was Geena Davis's husband when they made the loss-maker Cutthroat Island?\n",
    "Answer: Renny Harlin\n",
    "---\n",
    "Question: Who was married to Spandau Ballet's Gary Kemp and later to Jude Law?\n",
    "Answer: Sadie Frost\n",
    "\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "    },\n",
    "]\n",
    "PLAIN_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format_no_context, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    n=1,\n",
    "    top_p=0.9,\n",
    "    temperature=0,\n",
    "    repetition_penalty=1.2,\n",
    "    max_tokens=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145f037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference functions (no context used)\n",
    "def read_plain(llm, sampling_params, prompt_template, questions):\n",
    "    prompts = [prompt_template.format(question=q) for q in questions]\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    outputs = [output.outputs[0].text for output in outputs]\n",
    "    return outputs\n",
    "\n",
    "def answer_without_rag(instances):\n",
    "    questions = [instance[\"Question\"] for instance in instances]\n",
    "    question_ids = [instance[\"QuestionId\"] for instance in instances]\n",
    "\n",
    "    print(\"=> Generating answers without context...\")\n",
    "    answers = read_plain(model, sampling_params, PLAIN_PROMPT_TEMPLATE, questions)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    results = [{\"QuestionId\": qid, \"Answer\": answer} for qid, answer in zip(question_ids, answers)]\n",
    "    return results\n",
    "\n",
    "def plain_inference(validation_data):\n",
    "    predictions = {}\n",
    "    triviaqa_instances = {\n",
    "        \"Data\": [],\n",
    "        \"Domain\": \"Wikipedia\",\n",
    "        \"VerifiedEval\": False,\n",
    "        \"Version\": 1.0,\n",
    "    }\n",
    "\n",
    "    results = answer_without_rag(validation_data[\"Data\"])\n",
    "\n",
    "    for result in results:\n",
    "        predictions[result[\"QuestionId\"]] = result[\"Answer\"]\n",
    "\n",
    "    triviaqa_instances[\"Data\"].extend(validation_data[\"Data\"])\n",
    "    return predictions, triviaqa_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd7b93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answers without context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 500/500 [00:02<00:00, 248.63it/s, est. speed input: 62064.30 toks/s, output: 869.26 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Run plain inference\n",
    "predictions_no_context, triviaqa_instances_no_context = plain_inference(data)\n",
    "\n",
    "# Save files\n",
    "with open('test_results/triviaqa_test_instances_500_nocontext.json', 'w') as f:\n",
    "    json.dump(triviaqa_instances_no_context, f, indent=4)\n",
    "\n",
    "with open('test_results/triviaqa_test_predictions_nocontext.json', 'w') as f:\n",
    "    json.dump(predictions_no_context, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
